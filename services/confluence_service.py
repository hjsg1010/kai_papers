"""
Confluence and Markdown service for uploading paper analyses
"""
import logging
import re
import json
import requests
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Tuple

from config.settings import (
    CONFLUENCE_URL,
    CONFLUENCE_EMAIL,
    CONFLUENCE_API_TOKEN,
    CONFLUENCE_SPACE_KEY
)
from models import PaperAnalysis

logger = logging.getLogger(__name__)


# ===== Confluence Functions =====

def _conf_get_page_by_title(title: str) -> Optional[Dict]:
    """
    Confluence í˜ì´ì§€ë¥¼ ì œëª©ìœ¼ë¡œ ì¡°íšŒ

    Args:
        title: í˜ì´ì§€ ì œëª©

    Returns:
        í˜ì´ì§€ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None (í˜ì´ì§€ê°€ ì—†ëŠ” ê²½ìš°)
    """
    url = f"{CONFLUENCE_URL}/rest/api/content"
    params = {"title": title, "spaceKey": CONFLUENCE_SPACE_KEY, "expand": "version"}
    r = requests.get(url, params=params, auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=30)
    r.raise_for_status()
    res = r.json().get("results", [])
    return res[0] if res else None


def _conf_escape(s: str) -> str:
    """
    HTML íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„

    Args:
        s: ì´ìŠ¤ì¼€ì´í”„í•  ë¬¸ìì—´

    Returns:
        ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìì—´
    """
    return s.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")


def upload_to_confluence(analyses: List[PaperAnalysis], page_title: str):
    """
    ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ Confluenceì— ì—…ë¡œë“œ

    Args:
        analyses: PaperAnalysis ê°ì²´ ë¦¬ìŠ¤íŠ¸
        page_title: Confluence í˜ì´ì§€ ì œëª©

    Returns:
        ì—…ë¡œë“œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (success, page_url, page_id)

    Raises:
        Exception: Confluence API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    logger.info(f"Uploading to Confluence: {page_title}")
    body = [f"<h1>AI Paper Newsletter - {datetime.now().strftime('%Y-%m-%d')}</h1>",
            "<p>ì´ë²ˆ ì£¼ì˜ ì£¼ëª©í•  ë§Œí•œ AI ë…¼ë¬¸ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.</p>",
            '<ac:structured-macro ac:name="info"><ac:rich-text-body>',
            f"<p>ì´ {len(analyses)}í¸ì˜ ë…¼ë¬¸ì´ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.</p>",
            "</ac:rich-text-body></ac:structured-macro><hr/>"]
    for i, a in enumerate(analyses, 1):
        body.append(f"<h2>{i}. {_conf_escape(a.title)}</h2>")
        if a.authors:
            body.append(f"<p><strong>Authors:</strong> {_conf_escape(', '.join(a.authors[:8]))}</p>")
        if a.tags:
            body.append(f"<p><strong>Tags:</strong> {_conf_escape(', '.join(a.tags))}</p>")
        if a.abstract:
            body.append("<h3>Abstract</h3><p>" + _conf_escape(a.abstract) + "</p>")
        body.append("<h3>Analysis</h3>")
        body.append(a.summary)
        body.append(f"<p><em>Source:</em> s3://{a.source_file}</p>")
        body.append("<hr/>")
    content_html = "\n".join(body)

    create_url = f"{CONFLUENCE_URL}/rest/api/content"
    headers = {"Content-Type": "application/json"}
    try:
        existing = _conf_get_page_by_title(page_title)
        if existing:
            page_id = existing["id"]
            version = existing.get("version", {}).get("number", 1) + 1
            payload = {
                "id": page_id, "type": "page", "title": page_title,
                "space": {"key": CONFLUENCE_SPACE_KEY},
                "body": {"storage": {"value": content_html, "representation": "storage"}},
                "version": {"number": version},
            }
            r = requests.put(f"{create_url}/{page_id}", json=payload, headers=headers,
                             auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=60)
            r.raise_for_status()
            result = r.json()
        else:
            payload = {
                "type": "page", "title": page_title, "space": {"key": CONFLUENCE_SPACE_KEY},
                "body": {"storage": {"value": content_html, "representation": "storage"}},
            }
            r = requests.post(create_url, json=payload, headers=headers,
                              auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=60)
            r.raise_for_status()
            result = r.json()

        base = CONFLUENCE_URL.rstrip("/")
        webui = result.get("_links", {}).get("webui")
        tiny = result.get("_links", {}).get("tinyui")
        page_url = f"{base}{webui}" if webui else (f"{base}{tiny}" if tiny else f"{base}/pages/{result['id']}")
        logger.info(f"Confluence page: {page_url}")
        return {"success": True, "page_url": page_url, "page_id": result["id"]}
    except Exception as e:
        logger.exception("Confluence upload error")
        raise


# ===== Markdown Builder Functions =====

def derive_week_label(prefix: str) -> str:
    """
    prefixì—ì„œ ì£¼ì°¨ ë ˆì´ë¸”ì„ ì¶”ì¶œí•˜ê±°ë‚˜ í˜„ì¬ ì£¼ì°¨ë¥¼ ë°˜í™˜

    Args:
        prefix: S3 prefix ë˜ëŠ” ì£¼ì°¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ìì—´

    Returns:
        ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
    """
    m = re.search(r"w(\d{1,2})", prefix or "", re.IGNORECASE)
    if m:
        return f"w{int(m.group(1))}"
    iso_year, iso_week, _ = datetime.utcnow().isocalendar()
    return f"w{iso_week}"


def build_markdown(
    analyses: List[PaperAnalysis],
    papers_metadata: Optional[List[Dict]] = None,
    week_label: str = "",
    prefix: str = ""
) -> Tuple[str, str]:
    """
    ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜

    Args:
        analyses: PaperAnalysis ê°ì²´ ë¦¬ìŠ¤íŠ¸
        papers_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
        week_label: ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
        prefix: S3 prefix

    Returns:
        Tuple[str, str]: (íŒŒì¼ëª…, Markdown ì½˜í…ì¸ )
    """
    if not week_label:
        week_label = derive_week_label(prefix)

    header = f"""# AI Paper Newsletter â€“ {week_label}
_Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_

Source prefix: `{prefix}`

---

"""

    # ì´ë¯¸ì§€ ë§¤í•‘ ìƒì„±
    image_map = {}
    if papers_metadata:
        for meta in papers_metadata:
            if meta.get('images_info') and meta.get('images_info', {}).get('representative_images'):
                s3_key = meta.get('s3_key', '')
                image_map[s3_key] = meta['images_info']

    parts = [header]

    for i, a in enumerate(analyses, 1):
        tags = f"**Tags:** {', '.join(a.tags)}" if a.tags else ""
        authors = f"**Authors:** {', '.join(a.authors[:8])}" if a.authors else ""

        abstract_block = ""
        if a.abstract and a.abstract.strip():
            abstract_block = f"\n**Abstract**\n\n> {a.abstract.strip()}\n\n"

        # Summary JSON íŒŒì‹± ë° ê°œì¡°ì‹ ë³€í™˜
        summary_formatted = format_summary_as_markdown(a.summary)

        sec = f"""## {i}. {a.title}

{authors}
{tags}

{summary_formatted}

{abstract_block}"""

        # ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
        if a.source_file in image_map:
            img_info = image_map[a.source_file]
            rep_imgs = img_info.get('representative_images', [])

            if rep_imgs:
                rep_img = rep_imgs[0]
                paper_name = Path(a.source_file).stem
                img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{rep_img['type']}"

                sec += f"""### ğŸ“Š ëŒ€í‘œ ì´ë¯¸ì§€

**ì „ì²´ ì´ë¯¸ì§€:** {img_info['total_images']}ê°œ
**ëŒ€í‘œ ì´ë¯¸ì§€:** Figure {rep_img['index'] + 1} ({rep_img['size_kb']:.1f}KB)

![Figure {rep_img['index'] + 1}](images/{img_filename})

"""

        sec += f"""**Source:** `s3://{a.source_file}`

---

"""
        parts.append(sec)

    md_content = "".join(parts)
    md_filename = f"{week_label}.md"
    return md_filename, md_content


def format_summary_as_markdown(summary: str) -> str:
    """
    Summary JSONì„ ë³´ê¸° ì¢‹ì€ Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        summary: JSON í˜•íƒœì˜ summary ë¬¸ìì—´

    Returns:
        í¬ë§·íŒ…ëœ Markdown ë¬¸ìì—´
    """
    try:
        # JSON ì¶”ì¶œ ì‹œë„
        summary_clean = summary.strip().replace('~', 'â€“')

        # JSON íŒŒì‹±
        json_match = re.search(r'\{[\s\S]*\}', summary_clean)
        if not json_match:
            # JSONì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return f"**Summary**\n\n{summary_clean}\n"

        data = json.loads(json_match.group(0))

        # Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
        lines = ["**Summary**  \n\n"]

        # TL;DR
        if data.get('tldr'):
            lines.append(f"**ğŸ“Œ TL;DR**\n")
            lines.append(f"{data['tldr']}\n\n")

        # í•µì‹¬ ê¸°ì—¬
        if data.get('key_contributions'):
            lines.append(f"**ğŸ¯ í•µì‹¬ ê¸°ì—¬**\n")
            for contrib in data['key_contributions']:
                lines.append(f"- {contrib}\n")
            lines.append("\n")

        # ë°©ë²•ë¡ 
        if data.get('methodology'):
            lines.append(f"**ğŸ”¬ ë°©ë²•ë¡ **\n")
            lines.append(f"{data['methodology']}\n\n")

        # ê²°ê³¼
        if data.get('results'):
            lines.append(f"**ğŸ“Š ê²°ê³¼**\n")
            lines.append(f"{data['results']}\n\n")

        # ìƒˆë¡œìš´ ì 
        if data.get('novelty'):
            lines.append(f"**ğŸ’¡ ìƒˆë¡œìš´ ì **\n")
            lines.append(f"{data['novelty']}\n\n")

        # í•œê³„ì 
        if data.get('limitations'):
            lines.append(f"**âš ï¸ í•œê³„ì **\n")
            for limitation in data['limitations']:
                lines.append(f"- {limitation}\n")
            lines.append("\n")

        # Relevance Score
        if data.get('relevance_score'):
            score = data['relevance_score']
            stars = 'â­' * score
            lines.append(f"**ê´€ë ¨ì„± ì ìˆ˜:** {stars} ({score}/10)\n\n")

        return "".join(lines)

    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        logger.warning(f"Failed to parse summary JSON: {e}")
        return f"**Summary**\n\n{summary.strip().replace('~', 'â€“')}\n"
