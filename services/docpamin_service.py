"""
Docpamin Service for PDF Parsing

This module handles all interactions with the Docpamin API including:
- PDF parsing via file upload or URL
- Caching parsed results to S3
- Image extraction and processing
- Title extraction from markdown
"""

import logging
import json
import time
import requests
import zipfile
import io
import re
import hashlib
from typing import Dict, Tuple, Optional
from pathlib import Path
from urllib.parse import urlparse

# Import S3 client
from services.s3_service import s3_client

# Import image processing functions
from utils.image_processing import (
    process_markdown_images,
    select_representative_images,
    extract_figure_pairs_from_json,
    match_images_with_figure_pairs,
    match_images_with_captions_from_json,
    is_valid_caption
)

# Import configuration
from config.settings import (
    DOCPAMIN_API_KEY,
    DOCPAMIN_BASE_URL,
    DOCPAMIN_CRT_FILE,
    S3_BUCKET,
    S3_PAPERS_PREFIX
)

logger = logging.getLogger(__name__)


# ===== Cache Management =====

def get_docpamin_cache_key(source_identifier: str) -> str:
    """
    ìºì‹œ í‚¤ ìƒì„± (arXiv ID ë˜ëŠ” íŒŒì¼ í•´ì‹œ)

    Args:
        source_identifier: arXiv ID (ì˜ˆ: 2510.11701) ë˜ëŠ” S3 key

    Returns:
        ìºì‹œ í‚¤ (ì˜ˆ: "2510.11701" ë˜ëŠ” MD5 í•´ì‹œ)
    """
    # arXiv ID ì¶”ì¶œ
    arxiv_match = re.search(r'(\d{4}\.\d{5})', source_identifier)
    if arxiv_match:
        return arxiv_match.group(1)

    # ì¼ë°˜ íŒŒì¼ëª…ì—ì„œ í•´ì‹œ ìƒì„±
    return hashlib.md5(source_identifier.encode()).hexdigest()[:16]


def save_docpamin_cache_to_s3(
    bucket: str,
    prefix: str,
    cache_key: str,
    markdown: str,
    metadata: Dict
) -> bool:
    """
    Docpamin ê²°ê³¼ë¥¼ S3ì— ìºì‹±

    Args:
        bucket: S3 ë²„í‚·
        prefix: S3 prefix (ì˜ˆ: kai_papers/w44)
        cache_key: ìºì‹œ í‚¤
        markdown: íŒŒì‹±ëœ markdown
        metadata: JSON metadata

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        cache_prefix = f"{prefix.rstrip('/')}/cache"

        # Markdown ì €ì¥
        md_key = f"{cache_prefix}/{cache_key}.md"
        s3_client.put_object(
            Bucket=bucket,
            Key=md_key,
            Body=markdown.encode('utf-8'),
            ContentType='text/markdown'
        )
        logger.info(f"Saved markdown cache: s3://{bucket}/{md_key}")

        # Metadata ì €ì¥
        json_key = f"{cache_prefix}/{cache_key}.json"
        s3_client.put_object(
            Bucket=bucket,
            Key=json_key,
            Body=json.dumps(metadata, ensure_ascii=False).encode('utf-8'),
            ContentType='application/json'
        )
        logger.info(f"Saved metadata cache: s3://{bucket}/{json_key}")

        return True

    except Exception as e:
        logger.error(f"Failed to save Docpamin cache: {e}")
        return False


def load_docpamin_cache_from_s3(
    bucket: str,
    prefix: str,
    cache_key: str
) -> Tuple[Optional[str], Optional[Dict]]:
    """
    S3ì—ì„œ Docpamin ìºì‹œ ë¡œë“œ

    Args:
        bucket: S3 ë²„í‚·
        prefix: S3 prefix (ì˜ˆ: kai_papers/w44)
        cache_key: ìºì‹œ í‚¤

    Returns:
        (markdown, metadata) ë˜ëŠ” (None, None)
    """
    try:
        cache_prefix = f"{prefix.rstrip('/')}/cache"

        # Markdown ë¡œë“œ
        md_key = f"{cache_prefix}/{cache_key}.md"
        logger.info(f"Checking cache: s3://{bucket}/{md_key}")

        md_response = s3_client.get_object(Bucket=bucket, Key=md_key)
        markdown = md_response['Body'].read().decode('utf-8')

        # Metadata ë¡œë“œ
        json_key = f"{cache_prefix}/{cache_key}.json"
        json_response = s3_client.get_object(Bucket=bucket, Key=json_key)
        metadata = json.loads(json_response['Body'].read().decode('utf-8'))

        logger.info(f"âœ… Loaded from cache: {cache_key} (md_len={len(markdown)})")
        return markdown, metadata

    except s3_client.exceptions.NoSuchKey:
        logger.info(f"Cache not found: {cache_key}")
        return None, None
    except Exception as e:
        logger.error(f"Failed to load cache: {e}")
        return None, None


# ===== PDF Parsing =====

def parse_pdf_with_docpamin(pdf_path: str) -> Tuple[str, Dict]:
    """
    Docpamin APIë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì‹± (íŒŒì¼ ì—…ë¡œë“œ)

    Args:
        pdf_path: ë¡œì»¬ PDF íŒŒì¼ ê²½ë¡œ

    Returns:
        (cleaned_markdown, metadata): ì´ë¯¸ì§€ê°€ ì •ë¦¬ëœ ë§ˆí¬ë‹¤ìš´ê³¼ ë©”íƒ€ë°ì´í„°

    Raises:
        Exception: íŒŒì‹± ì‹¤íŒ¨ ì‹œ
    """
    logger.info(f"Parsing via Docpamin: {pdf_path}")
    headers = {"Authorization": f"Bearer {DOCPAMIN_API_KEY}"}
    session = requests.Session()
    session.headers.update(headers)
    REQ_TIMEOUT = 30

    try:
        with open(pdf_path, "rb") as f:
            files = {"file": f}
            data = {
                "alarm_options": json.dumps({"enabled": False}),
                "workflow_options": json.dumps({
                    "workflow": "dp-o1",
                    "image_export_mode": "embedded"
                }),
            }
            r = session.post(f"{DOCPAMIN_BASE_URL}/tasks", files=files, data=data,
                             verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
        r.raise_for_status()
        task_id = r.json().get("task_id")
        if not task_id:
            raise Exception("Docpamin: no task_id returned")

        logger.info(f"Docpamin task: {task_id}")
        max_wait, waited, backoff = 600, 0, 2
        while waited < max_wait:
            s = session.get(f"{DOCPAMIN_BASE_URL}/tasks/{task_id}",
                            verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
            s.raise_for_status()
            status = s.json().get("status")
            if status == "DONE":
                break
            if status in {"FAILED", "ERROR"}:
                raise Exception(f"Docpamin task failed: {status}")
            time.sleep(backoff)
            waited += backoff
            backoff = min(backoff * 1.5, 10)
        if waited >= max_wait:
            raise Exception("Docpamin timeout")

        opts = {"task_ids": [task_id], "output_types": ["markdown", "json"]}
        e = session.post(f"{DOCPAMIN_BASE_URL}/tasks/export", json=opts,
                         verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
        e.raise_for_status()

        md, meta = "", {}
        with zipfile.ZipFile(io.BytesIO(e.content)) as zf:
            for fn in zf.namelist():
                with zf.open(fn) as fh:
                    if fn.endswith(".md"):
                        s = fh.read().decode("utf-8", errors="ignore")
                        if len(s) > len(md):
                            md = s
                    elif fn.endswith(".json"):
                        try:
                            meta = json.loads(fh.read().decode("utf-8", errors="ignore"))
                        except Exception:
                            pass
        if not md:
            raise Exception("Docpamin: no markdown in export")

        paper_title = extract_title_from_markdown(md)
        meta['extracted_title'] = paper_title
        logger.info(f"Docpamin parsed OK (md_len={len(md)})")

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬: base64 ì œê±°, ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ
        md_cleaned, extracted_images = process_markdown_images(
            md,
            remove_for_llm=True,  # LLM ì…ë ¥ìš©ìœ¼ë¡œ base64 ì œê±°
            keep_representative=1
        )

        # ë©”íƒ€ë°ì´í„°ì— ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
        if extracted_images:
            representative = select_representative_images(
                extracted_images,
                max_count=1,
                paper_title=paper_title
            )
            meta['images_info'] = {
                'total_images': len(extracted_images),
                'representative_images': representative
            }
            logger.info(f"Image preprocessing: {len(extracted_images)} images, "
                       f"markdown size reduced from {len(md)} to {len(md_cleaned)} chars")

        return md_cleaned, meta
    except Exception as e:
        logger.error(f"Docpamin error: {e}")
        raise


def parse_pdf_with_docpamin_url(pdf_url: str, arxiv_id: str = "") -> Tuple[str, Dict]:
    """
    Docpamin APIë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì‹± (URL ê¸°ë°˜, ìºì‹± ì§€ì›)

    Args:
        pdf_url: PDF ë‹¤ìš´ë¡œë“œ URL
        arxiv_id: arXiv ID (ìºì‹±ìš©, ì„ íƒì‚¬í•­)

    Returns:
        (cleaned_markdown, metadata): ì´ë¯¸ì§€ê°€ ì •ë¦¬ëœ ë§ˆí¬ë‹¤ìš´ê³¼ ë©”íƒ€ë°ì´í„°

    Raises:
        Exception: íŒŒì‹± ì‹¤íŒ¨ ì‹œ

    Note:
        - S3 ìºì‹œë¥¼ ë¨¼ì € í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ Docpamin API í˜¸ì¶œ
        - JSON metadataì—ì„œ Figure-Caption ìŒì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ì— ë§¤ì¹­
        - Captionì´ ìˆëŠ” ì´ë¯¸ì§€ ì¤‘ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ
    """
    cache_key = get_docpamin_cache_key(arxiv_id or pdf_url)
    bucket = S3_BUCKET
    prefix = S3_PAPERS_PREFIX

    cached_md, cached_meta = load_docpamin_cache_from_s3(bucket, prefix, cache_key)

    if cached_md and cached_meta:
        logger.info(f"ğŸ“¦ Using cached Docpamin for {cache_key}")

        paper_title = extract_title_from_markdown(cached_md)
        cached_meta['extracted_title'] = paper_title

        # â­ ì´ë¯¸ì§€ë§Œ ì¶”ì¶œ (caption ì—†ìŒ)
        md_cleaned, extracted_images = process_markdown_images(
            cached_md,
            remove_for_llm=True
        )

        if extracted_images and cached_meta:
            # â­ JSONì—ì„œë§Œ caption ë§¤ì¹­!
            figure_pairs = extract_figure_pairs_from_json(cached_meta)

            if figure_pairs:
                extracted_images = match_images_with_figure_pairs(
                    extracted_images,
                    figure_pairs
                )

            # â­ Caption ìˆëŠ” ì´ë¯¸ì§€ë§Œ ì„ íƒ
            images_with_caption = [
                img for img in extracted_images
                if img.get('caption') and is_valid_caption(img.get('caption'))
            ]

            logger.info(f"Images with valid captions: {len(images_with_caption)}/{len(extracted_images)}")

            if images_with_caption:
                representative = select_representative_images(
                    images_with_caption,
                    max_count=1,
                    paper_title=paper_title
                )

                cached_meta['images_info'] = {
                    'total_images': len(extracted_images),
                    'images_with_caption': len(images_with_caption),
                    'representative_images': representative
                }

        return md_cleaned, cached_meta

    # ìºì‹œ ì—†ìŒ â†’ Docpamin íŒŒì‹±
    logger.info(f"Parsing via Docpamin (URL): {pdf_url}")
    headers = {"Authorization": f"Bearer {DOCPAMIN_API_KEY}"}
    session = requests.Session()
    session.headers.update(headers)
    REQ_TIMEOUT = 30

    try:
        data = {
            "file_url": pdf_url,
            "alarm_options": json.dumps({"enabled": False}),
            "workflow_options": json.dumps({
                "workflow": "dp-o1",
                "image_export_mode": "embedded"
            }),
        }

        r = session.post(
            f"{DOCPAMIN_BASE_URL}/tasks",
            data=data,
            verify=DOCPAMIN_CRT_FILE,
            timeout=REQ_TIMEOUT
        )
        r.raise_for_status()
        task_id = r.json().get("task_id")
        if not task_id:
            raise Exception("Docpamin: no task_id returned")

        logger.info(f"Docpamin task: {task_id}")

        # ìƒíƒœ í´ë§
        max_wait, waited, backoff = 600, 0, 2
        while waited < max_wait:
            s = session.get(
                f"{DOCPAMIN_BASE_URL}/tasks/{task_id}",
                verify=DOCPAMIN_CRT_FILE,
                timeout=REQ_TIMEOUT
            )
            s.raise_for_status()
            status = s.json().get("status")
            if status == "DONE":
                break
            if status in {"FAILED", "ERROR"}:
                raise Exception(f"Docpamin task failed: {status}")
            time.sleep(backoff)
            waited += backoff
            backoff = min(backoff * 1.5, 10)

        if waited >= max_wait:
            raise Exception("Docpamin timeout")

        # Export
        opts = {"task_ids": [task_id], "output_types": ["markdown", "json"]}
        e = session.post(
            f"{DOCPAMIN_BASE_URL}/tasks/export",
            json=opts,
            verify=DOCPAMIN_CRT_FILE,
            timeout=REQ_TIMEOUT
        )
        e.raise_for_status()

        md, meta = "", {}
        with zipfile.ZipFile(io.BytesIO(e.content)) as zf:
            for fn in zf.namelist():
                with zf.open(fn) as fh:
                    if fn.endswith(".md"):
                        s = fh.read().decode("utf-8", errors="ignore")
                        if len(s) > len(md):
                            md = s
                    elif fn.endswith(".json"):
                        try:
                            meta = json.loads(fh.read().decode("utf-8", errors="ignore"))
                        except Exception:
                            pass

        if not md:
            raise Exception("No markdown in export")

        # ìºì‹œ ì €ì¥
        save_docpamin_cache_to_s3(bucket, prefix, cache_key, md, meta)

        paper_title = extract_title_from_markdown(md)
        meta['extracted_title'] = paper_title
        meta['from_cache'] = False

        logger.info(f"Docpamin parsed (md_len={len(md)}, title={paper_title})")

        md_cleaned, extracted_images = process_markdown_images(
            md,
            remove_for_llm=True,
            keep_representative=1
        )

        if extracted_images:
            # â­ JSON metadata ì‚¬ìš©
            extracted_images = match_images_with_captions_from_json(
                extracted_images,
                meta
            )

            representative = select_representative_images(
                extracted_images,
                max_count=1,
                paper_title=paper_title
            )

            meta['images_info'] = {
                'total_images': len(extracted_images),
                'representative_images': representative
            }

        return md_cleaned, meta

    except Exception as e:
        logger.error(f"Docpamin error: {e}")
        raise


# ===== Title Extraction =====

def extract_title_from_url(url: str) -> str:
    """
    URLì—ì„œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ

    Args:
        url: arXiv URL (ì˜ˆ: https://arxiv.org/pdf/2312.12391.pdf)

    Returns:
        ì œëª© (arXiv ID ë˜ëŠ” íŒŒì¼ëª…)
    """
    # arXiv ID ì¶”ì¶œ
    match = re.search(r'(\d{4}\.\d{5})', url)
    if match:
        return match.group(1)

    # ì¼ë°˜ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
    path = urlparse(url).path
    return Path(path).stem or "unknown"


def extract_title_from_markdown(markdown: str) -> str:
    """
    Docpamin markdownì—ì„œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ

    Args:
        markdown: Docpaminì´ ë°˜í™˜í•œ markdown

    Returns:
        ë…¼ë¬¸ ì œëª© (ì²« ë²ˆì§¸ ## í—¤ë”©)
    """
    try:
        # ì²« ë²ˆì§¸ ## í—¤ë”© ì°¾ê¸°
        lines = markdown.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('##') and not line.startswith('###'):
                # ## ì œê±°í•˜ê³  ì œëª©ë§Œ ì¶”ì¶œ
                title = line.lstrip('#').strip()
                if title:
                    logger.info(f"Extracted title from markdown: {title}")
                    return title

        logger.warning("No title found in markdown")
        return "Unknown"

    except Exception as e:
        logger.error(f"Error extracting title: {e}")
        return "Unknown"
