#!/usr/bin/env python3

from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Tuple, Iterable
from datetime import datetime
from pathlib import Path
import boto3
from botocore.config import Config
import tempfile
import logging
import json
import time
import requests
import zipfile
import io
import re
import fnmatch
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import textwrap
import base64
import hashlib
from dotenv import load_dotenv, dotenv_values

load_dotenv(override=True)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===== Image Preprocessing Functions =====

def remove_base64_images(markdown: str, replacement: str = "[Image]") -> Tuple[str, int]:
    """
    Base64 ì´ë¯¸ì§€ë¥¼ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´
    
    Returns:
        (cleaned_markdown, num_removed)
    """
    pattern = r'!\[([^\]]*)\]\(data:image/[^;]+;base64,[A-Za-z0-9+/=]+\)'
    cleaned, count = re.subn(pattern, replacement, markdown)
    if count > 0:
        logger.info(f"Removed {count} base64 images from markdown")
    return cleaned, count

def extract_base64_images(markdown: str) -> List[Dict]:
    """Markdownì—ì„œ base64 ì´ë¯¸ì§€ ì¶”ì¶œ"""
    pattern = r'!\[([^\]]*)\]\(data:image/([^;]+);base64,([A-Za-z0-9+/=]+)\)'
    images = []
    for match in re.finditer(pattern, markdown):
        base64_data = match.group(3)
        size_bytes = len(base64_data) * 3 // 4
        images.append({
            'full_match': match.group(0),
            'alt_text': match.group(1),
            'mime_type': match.group(2),
            'base64_data': base64_data,
            'size_kb': size_bytes / 1024,
            'position': match.start()
        })
    return images

def select_representative_image(images: List[Dict], min_kb: float = 10, max_kb: float = 200) -> Optional[Dict]:
    """ëŒ€í‘œ ì´ë¯¸ì§€ ì„ ì • (í¬ê¸° + ìœ„ì¹˜ ê¸°ì¤€)"""
    if not images:
        return None
    candidates = [img for img in images if min_kb <= img['size_kb'] <= max_kb]
    if not candidates:
        candidates = sorted(images, key=lambda x: abs(x['size_kb'] - (min_kb + max_kb) / 2))[:3]
    return min(candidates, key=lambda x: x['position']) if candidates else None

app = FastAPI(
    title="AI Paper Newsletter Processor",
    description="Processes AI papers from S3, parses with Docpamin, summarizes via LLM. v2: Smart Hybrid Chunking (header-based or token-based)",
    version="2.0.0",
    docs_url="/docs",
    redoc_url=None,
    openapi_url="/openapi.json",
    root_path="/proxy/7070",
)

# ===== Configuration (env) =====
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
S3_BUCKET = os.getenv("S3_BUCKET_NAME")
S3_PAPERS_PREFIX = os.getenv("S3_PAPERS_PREFIX", "papers/")

DOCPAMIN_API_KEY = os.getenv("DOCPAMIN_API_KEY")
DOCPAMIN_BASE_URL = os.getenv("DOCPAMIN_BASE_URL", "https://docpamin.superaip.samsungds.net/api/v1")
DOCPAMIN_CRT_FILE = os.getenv("DOCPAMIN_CRT_FILE", "/etc/ssl/certs/ca-certificates.crt")

print(f"crt : {DOCPAMIN_CRT_FILE}")

LLM_API_KEY = os.getenv("LLM_API_KEY")
LLM_BASE_URL = os.getenv("LLM_BASE_URL")
LLM_MODEL = os.getenv("LLM_MODEL", "openai/gpt-oss-120b")
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "30000"))

CONFLUENCE_URL = os.getenv("CONFLUENCE_URL")
CONFLUENCE_EMAIL = os.getenv("CONFLUENCE_EMAIL")
CONFLUENCE_API_TOKEN = os.getenv("CONFLUENCE_API_TOKEN")
CONFLUENCE_SPACE_KEY = os.getenv("CONFLUENCE_SPACE_KEY")

MAX_WORKERS = int(os.getenv("MAX_WORKERS", "8"))

# ===== boto3 client =====
boto_config = Config(
    retries={"max_attempts": 5, "mode": "standard"},
    connect_timeout=10,
    read_timeout=60,
)
s3_client = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY,
    config=boto_config,
)

# ===== Models =====
class S3PapersRequest(BaseModel):
    bucket: Optional[str] = None
    prefix: Optional[str] = None
    file_pattern: Optional[str] = "*.pdf"
    process_subdirectories: bool = True
    week_label: Optional[str] = None
    upload_confluence: Optional[bool] = False
    # ê°œì„  ì˜µì…˜
    use_hierarchical: bool = True  # ê³„ì¸µì  ìš”ì•½ ì‚¬ìš©
    use_overlap: bool = True  # overlap chunking ì‚¬ìš©

class BatchProcessRequest(BaseModel):
    bucket: Optional[str] = None
    prefix: Optional[str] = None
    confluence_page_title: Optional[str] = None
    tags: List[str] = ["AI", "Research"]

class PaperAnalysis(BaseModel):
    title: str
    authors: List[str]
    abstract: str
    summary: str
    key_contributions: List[str]
    methodology: str
    results: str
    relevance_score: int
    tags: List[str]
    source_file: str

class DebugParseS3Request(BaseModel):
    bucket: Optional[str] = None
    key: str
    include_markdown: bool = False
    markdown_max_chars: int = 5000

class DebugSummarizeMarkdownRequest(BaseModel):
    title: str = "Untitled Paper"
    markdown: str
    include_section_summaries: bool = True
    include_final_analysis: bool = True
    return_markdown_preview_chars: int = 0
    # ê°œì„  ì˜µì…˜
    use_hierarchical: bool = True
    use_overlap: bool = True
    show_intermediate_steps: bool = False  # ì¤‘ê°„ ë‹¨ê³„ ì¶œë ¥

class DebugSummarizeSectionsRequest(BaseModel):
    title: str = "Untitled Paper"
    sections: Dict[str, str] = Field(default_factory=dict)
    only_sections: Optional[List[str]] = None
    # ê°œì„  ì˜µì…˜
    use_overlap: bool = True

# ===== Utilities =====
def _iter_s3_objects(bucket: str, prefix: str) -> Iterable[Dict]:
    paginator = s3_client.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            yield obj

def _preview(text: str, limit: int) -> str:
    if not text or limit <= 0:
        return ""
    if len(text) <= limit:
        return text
    return text[:limit] + "\n...\n(truncated)"

def get_paper_list_from_s3(bucket: str, prefix: str) -> Optional[List[str]]:
    """
    S3ì—ì„œ paper_list.txt íŒŒì¼ì„ ì½ì–´ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Args:
        bucket: S3 ë²„í‚·ëª…
        prefix: S3 prefix (ì˜ˆ: kai_papers/w43)
    
    Returns:
        URL ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None (íŒŒì¼ ì—†ìŒ)
    """
    paper_list_key = f"{prefix.rstrip('/')}/paper_list.txt"
    
    try:
        logger.info(f"Checking for paper list: s3://{bucket}/{paper_list_key}")
        
        response = s3_client.get_object(Bucket=bucket, Key=paper_list_key)
        content = response['Body'].read().decode('utf-8')
        
        # URL íŒŒì‹± (ë¹ˆ ì¤„, ì£¼ì„ ì œì™¸)
        urls = []
        for line in content.splitlines():
            line = line.strip()
            # ë¹ˆ ì¤„ì´ë‚˜ # ì£¼ì„ ì œì™¸
            if not line or line.startswith('#'):
                continue
            # URL í˜•ì‹ í™•ì¸
            if line.startswith('http'):
                urls.append(line)
            else:
                logger.warning(f"Invalid URL format: {line}")
        
        logger.info(f"Found {len(urls)} URLs in paper_list.txt")
        return urls if urls else None
        
    except s3_client.exceptions.NoSuchKey:
        logger.info(f"paper_list.txt not found: {paper_list_key}")
        return None
    except Exception as e:
        logger.error(f"Error reading paper_list.txt: {e}")
        return None

def get_s3_papers(
    bucket: str,
    prefix: str,
    file_pattern: str = "*.pdf",
    process_subdirectories: bool = True,
    min_size_bytes: int = 1024,
    max_size_bytes: int = 1024 * 1024 * 100,
) -> List[Dict]:
    logger.info(f"Fetching papers from S3: s3://{bucket}/{prefix}")
    papers = []
    for obj in _iter_s3_objects(bucket, prefix):
        key = obj["Key"]
        rel = key[len(prefix):].lstrip("/") if key.startswith(prefix) else key
        if not fnmatch.fnmatch(rel, file_pattern):
            continue
        if not process_subdirectories and "/" in rel:
            continue
        size = obj.get("Size", 0)
        if size < min_size_bytes or size > max_size_bytes:
            continue
        papers.append({
            "title": Path(key).stem.replace("_", " ").replace("-", " "),
            "s3_key": key,
            "s3_bucket": bucket,
            "last_modified": obj["LastModified"].isoformat(),
            "size_bytes": size,
            "source": "s3",
        })
    logger.info(f"Found {len(papers)} papers in S3")
    return papers

def download_pdf_from_s3(s3_key: str, s3_bucket: str) -> str:
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    try:
        logger.info(f"Downloading PDF: s3://{s3_bucket}/{s3_key}")
        s3_client.download_fileobj(s3_bucket, s3_key, tmp)
        tmp.close()
        return tmp.name
    except Exception as e:
        tmp.close()
        if os.path.exists(tmp.name):
            try: os.unlink(tmp.name)
            except Exception: pass
        raise Exception(f"Failed to download PDF: {e}")

# ===== Docpamin =====
def parse_pdf_with_docpamin(pdf_path: str) -> Tuple[str, Dict]:
    logger.info(f"Parsing via Docpamin: {pdf_path}")
    headers = {"Authorization": f"Bearer {DOCPAMIN_API_KEY}"}
    session = requests.Session()
    session.headers.update(headers)
    REQ_TIMEOUT = 30

    try:
        with open(pdf_path, "rb") as f:
            files = {"file": f}
            data = {
                "alarm_options": json.dumps({"enabled": False}),
                "workflow_options": json.dumps({
                    "workflow": "dp-o1",
                    "image_export_mode": "embedded"
                }),
            }
            r = session.post(f"{DOCPAMIN_BASE_URL}/tasks", files=files, data=data,
                             verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
        r.raise_for_status()
        task_id = r.json().get("task_id")
        if not task_id:
            raise Exception("Docpamin: no task_id returned")

        logger.info(f"Docpamin task: {task_id}")
        max_wait, waited, backoff = 600, 0, 2
        while waited < max_wait:
            s = session.get(f"{DOCPAMIN_BASE_URL}/tasks/{task_id}",
                            verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
            s.raise_for_status()
            status = s.json().get("status")
            if status == "DONE":
                break
            if status in {"FAILED", "ERROR"}:
                raise Exception(f"Docpamin task failed: {status}")
            time.sleep(backoff)
            waited += backoff
            backoff = min(backoff * 1.5, 10)
        if waited >= max_wait:
            raise Exception("Docpamin timeout")

        opts = {"task_ids": [task_id], "output_types": ["markdown", "json"]}
        e = session.post(f"{DOCPAMIN_BASE_URL}/tasks/export", json=opts,
                         verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
        e.raise_for_status()

        md, meta = "", {}
        with zipfile.ZipFile(io.BytesIO(e.content)) as zf:
            for fn in zf.namelist():
                with zf.open(fn) as fh:
                    if fn.endswith(".md"):
                        s = fh.read().decode("utf-8", errors="ignore")
                        if len(s) > len(md):
                            md = s
                    elif fn.endswith(".json"):
                        try:
                            meta = json.loads(fh.read().decode("utf-8", errors="ignore"))
                        except Exception:
                            pass
        if not md:
            raise Exception("Docpamin: no markdown in export")
        logger.info(f"Docpamin parsed OK (md_len={len(md)})")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬: base64 ì œê±°, ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ
        md_cleaned, extracted_images = process_markdown_images(
            md, 
            remove_for_llm=True,  # LLM ì…ë ¥ìš©ìœ¼ë¡œ base64 ì œê±°
            keep_representative=1
        )
        
        # ë©”íƒ€ë°ì´í„°ì— ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
        if extracted_images:
            meta['images_info'] = {
                'total_images': len(extracted_images),
                'representative_images': select_representative_images(extracted_images, max_count=1)
            }
            logger.info(f"Image preprocessing: {len(extracted_images)} images, "
                       f"markdown size reduced from {len(md)} to {len(md_cleaned)} chars")
        
        return md_cleaned, meta
    except Exception as e:
        logger.error(f"Docpamin error: {e}")
        raise

def parse_pdf_with_docpamin_url(pdf_url: str, arxiv_id: str = "") -> Tuple[str, Dict]:
    """
    URLì„ ì‚¬ìš©í•˜ì—¬ Docpaminìœ¼ë¡œ PDF íŒŒì‹±
    
    Args:
        pdf_url: PDF URL
        arxiv_id: arXiv ID (ì„ì‹œ ì œëª©ìš©)
    
    Returns:
        (markdown, metadata)
    """
    logger.info(f"Parsing via Docpamin (URL): {pdf_url}")
    headers = {"Authorization": f"Bearer {DOCPAMIN_API_KEY}"}
    session = requests.Session()
    session.headers.update(headers)
    REQ_TIMEOUT = 30

    try:
        data = {
            "file_url": pdf_url,
            "alarm_options": json.dumps({"enabled": False}),
            "workflow_options": json.dumps({
                "workflow": "dp-o1",
                "image_export_mode": "embedded"
            }),
        }
        
        r = session.post(
            f"{DOCPAMIN_BASE_URL}/tasks", 
            data=data,
            verify=DOCPAMIN_CRT_FILE, 
            timeout=REQ_TIMEOUT
        )
        r.raise_for_status()
        task_id = r.json().get("task_id")
        if not task_id:
            raise Exception("Docpamin: no task_id returned")

        logger.info(f"Docpamin task: {task_id}")
        
        # ìƒíƒœ í´ë§
        max_wait, waited, backoff = 600, 0, 2
        while waited < max_wait:
            s = session.get(
                f"{DOCPAMIN_BASE_URL}/tasks/{task_id}",
                verify=DOCPAMIN_CRT_FILE, 
                timeout=REQ_TIMEOUT
            )
            s.raise_for_status()
            status = s.json().get("status")
            if status == "DONE":
                break
            if status in {"FAILED", "ERROR"}:
                raise Exception(f"Docpamin task failed: {status}")
            time.sleep(backoff)
            waited += backoff
            backoff = min(backoff * 1.5, 10)
        
        if waited >= max_wait:
            raise Exception("Docpamin timeout")

        # Export
        opts = {"task_ids": [task_id], "output_types": ["markdown", "json"]}
        e = session.post(
            f"{DOCPAMIN_BASE_URL}/tasks/export", 
            json=opts,
            verify=DOCPAMIN_CRT_FILE, 
            timeout=REQ_TIMEOUT
        )
        e.raise_for_status()

        md, meta = "", {}
        with zipfile.ZipFile(io.BytesIO(e.content)) as zf:
            for fn in zf.namelist():
                with zf.open(fn) as fh:
                    if fn.endswith(".md"):
                        s = fh.read().decode("utf-8", errors="ignore")
                        if len(s) > len(md):
                            md = s
                    elif fn.endswith(".json"):
                        try:
                            meta = json.loads(fh.read().decode("utf-8", errors="ignore"))
                        except Exception:
                            pass
        
        if not md:
            raise Exception("Docpamin: no markdown in export")
        
        paper_title = extract_title_from_markdown(md)
        meta['extracted_title'] = paper_title
        
        logger.info(f"Docpamin parsed OK (md_len={len(md)}, title={paper_title})")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        md_cleaned, extracted_images = process_markdown_images(
            md, 
            remove_for_llm=True,
            keep_representative=1
        )
        
        if extracted_images:
            representative = select_representative_images(
                extracted_images, 
                max_count=1,
                paper_title=paper_title
            )
            
            meta['images_info'] = {
                'total_images': len(extracted_images),
                'representative_images': representative
            }
            logger.info(f"Image preprocessing: {len(extracted_images)} total, selected Figure {representative[0]['index']+1}")
        
        return md_cleaned, meta
        
    except Exception as e:
        logger.error(f"Docpamin error: {e}")
        raise

def extract_title_from_url(url: str) -> str:
    """
    URLì—ì„œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
    
    Args:
        url: arXiv URL (ì˜ˆ: https://arxiv.org/pdf/2312.12391.pdf)
    
    Returns:
        ì œëª© (arXiv ID)
    """
    # arXiv ID ì¶”ì¶œ
    match = re.search(r'(\d{4}\.\d{5})', url)
    if match:
        return match.group(1)
    
    # ì¼ë°˜ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
    from urllib.parse import urlparse
    path = urlparse(url).path
    return Path(path).stem or "unknown"

def extract_title_from_markdown(markdown: str) -> str:
    """
    Docpamin markdownì—ì„œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
    
    Args:
        markdown: Docpaminì´ ë°˜í™˜í•œ markdown
    
    Returns:
        ë…¼ë¬¸ ì œëª© (ì²« ë²ˆì§¸ ## í—¤ë”©)
    """
    try:
        # ì²« ë²ˆì§¸ ## í—¤ë”© ì°¾ê¸°
        lines = markdown.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('##') and not line.startswith('###'):
                # ## ì œê±°í•˜ê³  ì œëª©ë§Œ ì¶”ì¶œ
                title = line.lstrip('#').strip()
                if title:
                    logger.info(f"Extracted title from markdown: {title}")
                    return title
        
        logger.warning("No title found in markdown")
        return "Unknown"
        
    except Exception as e:
        logger.error(f"Error extracting title: {e}")
        return "Unknown"

# ===== Image Processing =====
def process_markdown_images(
    markdown: str, 
    remove_for_llm: bool = True,
    keep_representative: int = 1
) -> Tuple[str, List[Dict]]:
    """
    Markdownì—ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬
    
    Args:
        markdown: ì›ë³¸ markdown (base64 ì´ë¯¸ì§€ í¬í•¨)
        remove_for_llm: LLM ìš”ì•½ìš©ìœ¼ë¡œ ì´ë¯¸ì§€ ì œê±° ì—¬ë¶€
        keep_representative: ìµœì¢… ê²°ê³¼ë¬¼ì— í¬í•¨í•  ëŒ€í‘œ ì´ë¯¸ì§€ ê°œìˆ˜
    
    Returns:
        (processed_markdown, extracted_images)
    """
    # Base64 ì´ë¯¸ì§€ íŒ¨í„´: ![alt](data:image/png;base64,...)
    pattern = r'!\[(.*?)\]\(data:image/([^;]+);base64,([A-Za-z0-9+/=]+)\)'
    
    images = []
    
    def extract_image(match):
        alt_text = match.group(1)
        img_type = match.group(2)
        base64_data = match.group(3)
        
        full_img = match.group(0)
        img_size = len(base64_data)
                
        images.append({
            'index': len(images),
            'alt': alt_text.strip(),
            'type': img_type,
            'size': img_size,
            'size_kb': img_size * 3 / 4 / 1024,  # â­ ì¶”ê°€!
            'base64_data': base64_data,          # â­ ì¶”ê°€!
            'full': full_img
        })
        
        if remove_for_llm:
            # LLMìš©: ì´ë¯¸ì§€ë¥¼ ìº¡ì…˜ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´
            if alt_text.strip():
                return f"\n[Figure {len(images)}: {alt_text}]\n"
            else:
                return f"\n[Figure {len(images)}]\n"
        else:
            # ì›ë³¸ ìœ ì§€
            return full_img
    
    # ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì²˜ë¦¬
    processed_md = re.sub(pattern, extract_image, markdown)
    
    if images:
        total_img_size = sum(img['size'] for img in images)
        logger.info(f"Processed {len(images)} images. "
                   f"Total image data: {total_img_size:,} chars")
        logger.info(f"Size reduction: {len(markdown) - len(processed_md):,} chars "
                   f"({100 * (1 - len(processed_md) / len(markdown)):.1f}%)")
    
    return processed_md, images

def select_representative_image_with_llm(images: List[Dict], paper_title: str = "") -> Dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ëŒ€í‘œì ì¸ ì´ë¯¸ì§€ ì„ íƒ
    
    Args:
        images: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (ê°ê° index, caption, size_kb í¬í•¨)
        paper_title: ë…¼ë¬¸ ì œëª© (ì»¨í…ìŠ¤íŠ¸ìš©)
    
    Returns:
        ì„ íƒëœ ì´ë¯¸ì§€ ë”•ì…”ë„ˆë¦¬
    """
    if not images:
        return None
    
    if len(images) == 1:
        return images[0]
    
    try:
        # ì´ë¯¸ì§€ ì •ë³´ í¬ë§·
        image_descriptions = []
        for img in images:
            desc = f"Figure {img['index'] + 1}"
            if img.get('alt') and img['alt'] != 'Image':
                desc += f": {img['alt']}"
            desc += f" (Size: {img['size_kb']:.1f}KB)"
            image_descriptions.append(desc)
        
        # í”„ë¡¬í”„íŠ¸
        prompt = f"""You are analyzing a research paper titled: "{paper_title}"

Below is a list of figures from this paper. Select the ONE figure that best represents the main contribution or overview of the paper. 

Prioritize figures that show:
1. Overall architecture/framework diagrams
2. System overview illustrations
3. Main workflow diagrams

Avoid selecting:
- Detailed experimental result graphs
- Comparison tables
- Ablation study charts
- Small component diagrams

Figures:
{chr(10).join(f"{i+1}. {desc}" for i, desc in enumerate(image_descriptions))}

Respond with ONLY the number (1-{len(images)}) of the best representative figure. No explanation needed."""

        # call_llm ì‚¬ìš©
        messages = [{"role": "user", "content": prompt}]
        response = call_llm(messages, max_tokens=10)
        
        # ìˆ«ì ì¶”ì¶œ
        response_text = response.strip()
        numbers = re.findall(r'\d+', response_text)
        
        if not numbers:
            raise ValueError(f"No number found in LLM response: {response_text}")
        
        selected_num = int(numbers[0])
        selected_idx = selected_num - 1
        
        if 0 <= selected_idx < len(images):
            logger.info(f"LLM selected Figure {selected_num} as representative image (from response: '{response_text}')")
            return images[selected_idx]
        else:
            logger.warning(f"LLM returned invalid index: {selected_num} (valid: 1-{len(images)}), using largest")
            return max(images, key=lambda x: x['size_kb'])
            
    except Exception as e:
        logger.error(f"LLM image selection failed: {e}, falling back to size-based")
        return max(images, key=lambda x: x['size_kb'])


def select_representative_images(images: List[Dict], max_count: int = 1, paper_title: str = "") -> List[Dict]:
    """
    ë…¼ë¬¸ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ (LLM ê¸°ë°˜)
    
    Args:
        images: ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
        max_count: ìµœëŒ€ ì„ íƒ ê°œìˆ˜ (í˜„ì¬ëŠ” 1ê°œë§Œ)
        paper_title: ë…¼ë¬¸ ì œëª©
    
    Returns:
        ì„ íƒëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
    """
    if not images:
        return []
    
    if len(images) <= max_count:
        return images[:max_count]
    
    # LLMìœ¼ë¡œ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ
    selected = select_representative_image_with_llm(images, paper_title)
    return [selected] if selected else []


# ===== LLM utils =====
def _estimate_tokens(s: str) -> int:
    """ê°„ë‹¨í•œ í† í° ì¶”ì • (1 token â‰ˆ 4 chars)"""
    return max(1, math.ceil(len(s) / 4))

def call_llm(messages: List[Dict], max_tokens: int = 2000) -> str:
    """LLM API í˜¸ì¶œ"""
    headers = {"Authorization": f"Bearer {LLM_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": LLM_MODEL, "messages": messages, "max_tokens": max_tokens, "temperature": 0.2}
    try:
        # trailing slash ì œê±°í•˜ì—¬ ì´ì¤‘ ìŠ¬ë˜ì‹œ ë°©ì§€
        base_url = LLM_BASE_URL.rstrip('/') if LLM_BASE_URL else ""
        url = f"{base_url}/chat/completions"
        
        logger.info(f"Calling LLM: {url} (model: {LLM_MODEL})")
        
        r = requests.post(url, headers=headers, json=payload, timeout=120)
        r.raise_for_status()
        j = r.json()
        return j["choices"][0]["message"]["content"]
    except requests.exceptions.HTTPError as e:
        logger.error(f"LLM API HTTP Error: {e.response.status_code} - {e.response.text[:200]}")
        raise
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        raise

def extract_all_headers(markdown_content: str) -> Dict[str, str]:
    """
    ì„¹ì…˜ ì´ë¦„ì— ê´€ê³„ì—†ì´ ì£¼ìš” í—¤ë”ë§Œ ì¶”ì¶œí•˜ì—¬ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬
    # (Level 1) ê³¼ ## (Level 2) í—¤ë”ë§Œ ì‚¬ìš© (### ì œì™¸)
    """
    logger.info("Extracting main headers (# and ##) from markdown...")
    sections = []
    current_section = {"header": "preamble", "level": 0, "content": []}
    
    for line in markdown_content.splitlines():
        # í—¤ë” ê°ì§€ (# ë˜ëŠ” ## ë§Œ, ### ì œì™¸!)
        header_match = re.match(r'^(#{1,2})\s+(.+)$', line.strip())
        
        if header_match:
            # ì´ì „ ì„¹ì…˜ ì €ì¥
            if current_section["content"]:
                sections.append(current_section)
            
            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            level = len(header_match.group(1))
            header = header_match.group(2).strip()
            current_section = {
                "header": header,
                "level": level,
                "content": []
            }
        else:
            current_section["content"].append(line)
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
    if current_section["content"]:
        sections.append(current_section)
    
    # Dictë¡œ ë³€í™˜ (í‚¤: section_N_header)
    result = {}
    for i, sec in enumerate(sections):
        # í—¤ë”ë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ë²ˆí˜¸ ì¶”ê°€)
        header_clean = re.sub(r'[^\w\s-]', '', sec['header'])[:30]
        key = f"section_{i:02d}_{header_clean}"
        content = "\n".join(sec["content"])
        
        if content.strip():  # ë¹ˆ ì„¹ì…˜ ì œì™¸
            result[key] = content
            logger.info(f"Section '{sec['header']}': {len(content)} chars, ~{_estimate_tokens(content)} toks")
    
    return result

def chunk_by_tokens(text: str, chunk_size: int = 4000, overlap: int = 400) -> List[str]:
    """
    í† í° ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê· ë“± ë¶„í• 
    - ì„¹ì…˜ êµ¬ì¡°ê°€ ì—†ê±°ë‚˜ ë¶ˆëª…í™•í•  ë•Œ ì‚¬ìš©
    """
    logger.info(f"Chunking by tokens (chunk_size={chunk_size}, overlap={overlap})...")
    
    total_chars = len(text)
    if total_chars <= chunk_size * 4:  # í•œ ì²­í¬ì— ë“¤ì–´ê°
        return [text]
    
    # ëŒ€ëµì ì¸ ë¬¸ì ìˆ˜ ê³„ì‚° (1 token â‰ˆ 4 chars)
    char_per_chunk = chunk_size * 4
    overlap_chars = overlap * 4
    
    chunks = []
    start = 0
    
    while start < total_chars:
        end = start + char_per_chunk
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        if end < total_chars:
            # ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ìë¥´ê¸°
            for i in range(end, max(start + char_per_chunk // 2, end - 500), -1):
                if i < len(text) and text[i] in '.!?\n':
                    end = i + 1
                    break
        
        chunk = text[start:end]
        chunks.append(chunk)
        
        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (overlap ì ìš©)
        start = end - overlap_chars
        
        if start >= total_chars:
            break
    
    logger.info(f"Created {len(chunks)} chunks by tokens")
    return chunks

def smart_chunk_hybrid(markdown_content: str, min_headers: int = 10) -> Tuple[Dict[str, str], str]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹: í—¤ë” ê°œìˆ˜ì— ë”°ë¼ ë°©ì‹ ì„ íƒ
    
    ê¸°ë³¸ê°’ì„ 8â†’10ìœ¼ë¡œ ìƒí–¥ ì¡°ì •í•˜ì—¬ ì²­í¬ ìˆ˜ ë”ìš± ê°ì†Œ
    # (Level 1) í—¤ë”ë§Œ ì¹´ìš´íŠ¸ (## ì œì™¸)
    
    Returns:
        (chunks_dict, method_used)
        method_used: "header_based" or "token_based"
    """
    # ë©”ì¸ í—¤ë”ë§Œ ì¹´ìš´íŠ¸ (# ë§Œ, ## ì œì™¸)
    main_headers = re.findall(r'^#\s+[^#].+$', markdown_content, re.MULTILINE)
    num_headers = len(main_headers)
    
    logger.info(f"Found {num_headers} main headers (# only) in markdown")
    
    if num_headers >= min_headers:
        # ì¶©ë¶„í•œ í—¤ë” â†’ í—¤ë” ê¸°ë°˜ ì„¹ì…˜ ë¶„ë¦¬ (í•˜ì§€ë§Œ ê±°ì˜ ì—†ì„ ê²ƒ)
        logger.info(f"Using HEADER-BASED chunking ({num_headers} headers >= {min_headers})")
        chunks = extract_all_headers(markdown_content)
        return chunks, "header_based"
    else:
        # í—¤ë” ë¶€ì¡± â†’ í† í° ê¸°ë°˜ ê· ë“± ë¶„í•  (ëŒ€ë¶€ë¶„ ì´ìª½)
        logger.info(f"Using TOKEN-BASED chunking ({num_headers} headers < {min_headers})")
        chunk_list = chunk_by_tokens(markdown_content, chunk_size=6000, overlap=600)
        chunks = {f"chunk_{i:02d}": chunk for i, chunk in enumerate(chunk_list)}
        return chunks, "token_based"

# ===== ê°œì„ ëœ ìš”ì•½ í•¨ìˆ˜ë“¤ =====

def smart_chunk_with_overlap(text: str, chunk_size: int = 4000, overlap: int = 400) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ overlapì„ ê°€ì§€ê³  ì²­í¬ë¡œ ë¶„í• 
    - chunk_size: ê° ì²­í¬ì˜ ëŒ€ëµì ì¸ í¬ê¸° (ë¬¸ì ë‹¨ìœ„)
    - overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (ë¬¸ì ë‹¨ìœ„)
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ overlap ì ìš©
        if end < len(text):
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            chunk_end = end
            # ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì„ ì°¾ì•„ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ ì°¾ê¸°
            for i in range(end, max(start + chunk_size // 2, end - 200), -1):
                if text[i] in '.!?\n':
                    chunk_end = i + 1
                    break
            chunks.append(text[start:chunk_end])
            start = chunk_end - overlap  # overlapë§Œí¼ ë’¤ë¡œ
        else:
            chunks.append(text[start:])
            break
    
    logger.info(f"Split into {len(chunks)} chunks with overlap={overlap}")
    return chunks

def summarize_chunk_with_overlap(
    chunk_key: str, 
    chunk_content: str, 
    paper_title: str,
    use_overlap: bool = True,
    prev_summary: str = ""
) -> str:
    """
    ì²­í¬ë¥¼ overlapì„ ê°€ì§€ê³  ìš”ì•½ (ì„¹ì…˜ ì´ë¦„ ë¬´ê´€)
    - chunk_key: "section_01_abstract" ë˜ëŠ” "chunk_00" í˜•ì‹
    - prev_summary: ì´ì „ ì²­í¬ì˜ ìš”ì•½ (contextë¡œ ì‚¬ìš©)
    """
    if not chunk_content.strip():
        return ""
    
    # ì²­í¬ íƒ€ì…ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    if chunk_key.startswith("section_"):
        # ì„¹ì…˜ ê¸°ë°˜: í—¤ë” ì´ë¦„ ì¶”ì¶œ
        header_part = chunk_key.split("_", 2)[-1] if "_" in chunk_key else "content"
        prompt = f"ë‹¤ìŒ '{header_part}' ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”."
    else:
        # í† í° ê¸°ë°˜: ìˆœì„œë§Œ í‘œì‹œ
        chunk_num = chunk_key.split("_")[-1] if "_" in chunk_key else "0"
        prompt = f"ë…¼ë¬¸ì˜ ì¼ë¶€ë¶„ (Part {chunk_num})ì„ ìš”ì•½í•˜ì„¸ìš”."
    
    budget_tokens = min(LLM_MAX_TOKENS - 800, 3000)
    approx_tokens = _estimate_tokens(chunk_content)
    
    # ì²­í¬ê°€ ë„ˆë¬´ í¬ë©´ ë‹¤ì‹œ ë¶„í• 
    if approx_tokens <= budget_tokens:
        # í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
        context_prompt = ""
        if prev_summary and use_overlap:
            context_prompt = f"\nì´ì „ ë‚´ìš© ìš”ì•½: {prev_summary}\n"
        
        msgs = [
            {"role": "system", "content": "You are an expert AI paper analyst. Keep technical terms in English."},
            {"role": "user", "content": f"[{paper_title}] {prompt}{context_prompt}\n\në‚´ìš©:\n{chunk_content}"},
        ]
        return call_llm(msgs, max_tokens=min(1500, LLM_MAX_TOKENS - 500))
    
    # ë„ˆë¬´ í¬ë©´ sub-chunkë¡œ ë¶„í• 
    chunk_size_chars = budget_tokens * 4
    overlap_chars = 400 if use_overlap else 0
    
    sub_chunks = smart_chunk_with_overlap(chunk_content, chunk_size_chars, overlap_chars)
    
    summaries = []
    sub_prev_summary = prev_summary
    
    for i, sub_chunk in enumerate(sub_chunks):
        context_prompt = ""
        if sub_prev_summary and use_overlap:
            context_prompt = f"\nì´ì „ ë‚´ìš©: {sub_prev_summary}\n"
        
        msgs = [
            {"role": "system", "content": "You are an expert AI paper analyst. Keep technical terms in English."},
            {"role": "user", "content": f"[{paper_title}] {prompt} (sub-part {i+1}/{len(sub_chunks)}){context_prompt}\n\n{sub_chunk}"},
        ]
        summary = call_llm(msgs, max_tokens=min(1500, LLM_MAX_TOKENS - 500))
        summaries.append(summary)
        sub_prev_summary = summary
    
    # ì—¬ëŸ¬ sub-chunk ìš”ì•½ì„ ë³‘í•©
    if len(summaries) == 1:
        return summaries[0]
    
    merge_msgs = [
        {"role": "system", "content": "You are an expert AI paper analyst."},
        {"role": "user", "content": f"ë‹¤ìŒì€ [{paper_title}]ì˜ '{chunk_key}' ë¶€ë¶„ì„ ì—¬ëŸ¬ sub-partë¡œ ë‚˜ëˆ  ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì˜ ì¼ê´€ëœ ìš”ì•½ìœ¼ë¡œ ë³‘í•©í•˜ì„¸ìš”:\n\n" + "\n\n---\n\n".join(summaries)},
    ]
    return call_llm(merge_msgs, max_tokens=1200)

def create_hierarchical_summary_v2(chunk_summaries: Dict[str, str], paper_title: str) -> Dict[str, str]:
    """
    ê³„ì¸µì  ìš”ì•½ ìƒì„± v2 (ìœ„ì¹˜ ê¸°ë°˜)
    Level 1: ì²­í¬ ìš”ì•½ (ì´ë¯¸ ì™„ë£Œ)
    Level 2: ìœ„ì¹˜ë³„ ê·¸ë£¹ ìš”ì•½ (beginning, middle, end)
    Level 3: ìµœì¢… í†µí•© ìš”ì•½
    
    ì„¹ì…˜ ì´ë¦„ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë…¼ë¬¸ì˜ ìœ„ì¹˜(ì•/ì¤‘ê°„/ë’¤)ë¡œ ê·¸ë£¹í•‘
    """
    logger.info("Creating hierarchical summary (position-based)...")
    
    num_chunks = len(chunk_summaries)
    if num_chunks == 0:
        return {}
    
    # Level 2: ìœ„ì¹˜ ê¸°ë°˜ ê·¸ë£¹í•‘
    items = list(chunk_summaries.items())
    
    if num_chunks <= 2:
        # ì²­í¬ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        groups = {"full": items}
    elif num_chunks <= 5:
        # ì²­í¬ê°€ ì ìœ¼ë©´ 2ê°œ ê·¸ë£¹
        mid_point = num_chunks // 2
        groups = {
            "beginning": items[:mid_point],
            "end": items[mid_point:]
        }
    else:
        # ì²­í¬ê°€ ë§ìœ¼ë©´ 3ê°œ ê·¸ë£¹
        third = num_chunks // 3
        groups = {
            "beginning": items[:third],
            "middle": items[third:third*2],
            "end": items[third*2:]
        }
    
    # ê·¸ë£¹ë³„ í”„ë¡¬í”„íŠ¸
    group_prompts = {
        "full": "ë…¼ë¬¸ì˜ ì „ì²´ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
        "beginning": "ë…¼ë¬¸ì˜ ë„ì…ë¶€ (ë°°ê²½, ë¬¸ì œ ì •ì˜, ëª©í‘œ, ê´€ë ¨ ì—°êµ¬)ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
        "middle": "ë…¼ë¬¸ì˜ í•µì‹¬ ë¶€ë¶„ (ë°©ë²•ë¡ , ì‹¤í—˜ ì„¤ê³„, ê²°ê³¼, ì„±ëŠ¥)ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
        "end": "ë…¼ë¬¸ì˜ ê²°ë¡  ë¶€ë¶„ (ì¸ì‚¬ì´íŠ¸, í•œê³„, ê¸°ì—¬, í–¥í›„ ê³¼ì œ)ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”."
    }
    
    intermediate_summaries = {}
    for group_name, group_items in groups.items():
        if not group_items:
            continue
        
        # ê·¸ë£¹ ë‚´ ìš”ì•½ë“¤ì„ ê²°í•©
        group_texts = [f"### Part {i+1}\n{summary}" for i, (key, summary) in enumerate(group_items)]
        combined = "\n\n".join(group_texts)
        
        prompt = group_prompts.get(group_name, "ë‹¤ìŒ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.")
        
        msgs = [
            {"role": "system", "content": "You are an expert AI paper analyst."},
            {"role": "user", "content": f"[{paper_title}] {prompt}\n\n{combined}"},
        ]
        
        intermediate_summaries[group_name] = call_llm(msgs, max_tokens=1000)
        logger.info(f"Created intermediate summary for '{group_name}' ({len(group_items)} chunks)")
    
    return intermediate_summaries

def _json_extract(s: str) -> Optional[Dict]:
    """ë¬¸ìì—´ì—ì„œ JSON ì¶”ì¶œ"""
    m = re.search(r"\{[\s\S]*\}", s)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None

def analyze_paper_with_llm_improved(
    paper_info: Dict, 
    markdown_content: str, 
    json_metadata: Dict,
    use_hierarchical: bool = True,
    use_overlap: bool = True,
    return_intermediate: bool = False
) -> Tuple[PaperAnalysis, Optional[Dict]]:
    """
    ê°œì„ ëœ ë…¼ë¬¸ ë¶„ì„ í•¨ìˆ˜ v2 (í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹)
    - í—¤ë” ë§ìœ¼ë©´ â†’ ì„¹ì…˜ ê¸°ë°˜
    - í—¤ë” ì ìœ¼ë©´ â†’ í† í° ê¸°ë°˜
    - use_hierarchical: ê³„ì¸µì  ìš”ì•½ ì‚¬ìš©
    - use_overlap: overlap chunking ì‚¬ìš©
    - return_intermediate: ì¤‘ê°„ ë‹¨ê³„ ê²°ê³¼ ë°˜í™˜
    """
    logger.info(f"Analyzing paper (hierarchical={use_hierarchical}, overlap={use_overlap}): {paper_info.get('title','Unknown')}")
    
    # Step 0: ì´ë¯¸ì§€ ì²˜ë¦¬ (LLM í† í° ì ˆì•½)
    clean_markdown, extracted_images = process_markdown_images(
        markdown_content, 
        remove_for_llm=True,
        keep_representative=1
    )
    
    if extracted_images:
        logger.info(f"Removed {len(extracted_images)} images for LLM processing")
    
    # Step 1: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (í•˜ì´ë¸Œë¦¬ë“œ) - clean_markdown ì‚¬ìš©
    chunks, chunking_method = smart_chunk_hybrid(clean_markdown, min_headers=10)
    logger.info(f"Chunking method: {chunking_method}, total chunks: {len(chunks)}")
    
    # Step 2: ê° ì²­í¬ ìš”ì•½
    chunk_summaries: Dict[str, str] = {}
    prev_summary = ""
    
    for chunk_key, content in chunks.items():
        if content.strip() and len(content.strip()) > 100:  # 100ì ì´ìƒë§Œ
            summary = summarize_chunk_with_overlap(
                chunk_key, 
                content, 
                paper_info.get("title", "Unknown"),
                use_overlap=use_overlap,
                prev_summary=prev_summary if use_overlap else ""
            )
            chunk_summaries[chunk_key] = summary
            prev_summary = summary
    
    logger.info(f"Created {len(chunk_summaries)} chunk summaries")
    
    # Step 3: ê³„ì¸µì  ìš”ì•½ (ì˜µì…˜)
    intermediate_summaries = {}
    if use_hierarchical and len(chunk_summaries) > 0:
        intermediate_summaries = create_hierarchical_summary_v2(
            chunk_summaries, 
            paper_info.get("title", "Unknown")
        )
        # ê³„ì¸µì  ìš”ì•½ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë¶„ì„
        combined = "\n\n".join([f"## {k.title()}\n{v}" for k, v in intermediate_summaries.items() if v.strip()])
    else:
        # ê¸°ì¡´ ë°©ì‹: ì²­í¬ ìš”ì•½ì„ ì§ì ‘ ê²°í•©
        combined = "\n\n".join([f"## {k}\n{v}" for k, v in chunk_summaries.items() if v.strip()])
    
    # Step 4: ìµœì¢… ì¢…í•© ë¶„ì„
    format_hint = {
        "title": paper_info.get("title", "Unknown"),
        "tldr": "",
        "key_contributions": [],
        "methodology": "",
        "results": "",
        "novelty": "",
        "limitations": [],
        "relevance_score": 7,
        "tags": [],
    }
    
    final_prompt = f"""ë…¼ë¬¸ "{paper_info.get('title','Unknown')}"ì˜ {'ê³„ì¸µì  ' if use_hierarchical else ''}ìš”ì•½ì´ ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤:

{combined}

ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ê²°ê³¼ë§Œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”(ì„¤ëª…ë¬¸ ê¸ˆì§€):
{json.dumps(format_hint, ensure_ascii=False, indent=2)}

ê·œì¹™:
- key_contributions: 3~6ê°œ bullet ìˆ˜ì¤€ì˜ ê°„ê²° ë¬¸ì¥
- relevance_score: 1~10 ì •ìˆ˜
- tags: 5~8ê°œ ì§§ì€ í‘œì œì–´ (ì˜ë¬¸)
- í•œê¸€ë¡œ ì‘ì„±
- ì „ë¬¸ ìš©ì–´ëŠ” English ê·¸ëŒ€ë¡œ ìœ ì§€
"""
    
    msgs = [
        {"role": "system", "content": "You are an expert AI/ML researcher. Return ONLY valid JSON."},
        {"role": "user", "content": final_prompt},
    ]
    final_out = call_llm(msgs, max_tokens=min(2500, LLM_MAX_TOKENS))
    parsed = _json_extract(final_out) or {}
    
    # Abstract ì¶”ì¶œ (ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ)
    abstract_text = ""
    for key, content in chunks.items():
        if len(content) < 2000:  # abstractëŠ” ë³´í†µ ì§§ìŒ
            abstract_text = content[:800]
            break
    
    analysis = PaperAnalysis(
        title=paper_info.get("title", "Unknown"),
        authors=paper_info.get("authors", []),
        abstract=abstract_text if abstract_text else "",
        summary=final_out if isinstance(final_out, str) else json.dumps(final_out, ensure_ascii=False),
        key_contributions=parsed.get("key_contributions", []),
        methodology=parsed.get("methodology", ""),
        results=parsed.get("results", ""),
        relevance_score=int(parsed.get("relevance_score", 7)),
        tags=parsed.get("tags", []),
        source_file=paper_info.get("s3_key", ""),
    )
    
    # ì¤‘ê°„ ë‹¨ê³„ ê²°ê³¼
    intermediate_data = None
    if return_intermediate:
        # ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ
        representative_imgs = select_representative_images(extracted_images, max_count=1)
        
        intermediate_data = {
            "chunking_method": chunking_method,
            "num_chunks": len(chunks),
            "chunks_detected": list(chunks.keys()),
            "chunk_summaries": chunk_summaries,
            "intermediate_summaries": intermediate_summaries if use_hierarchical else {},
            "images": {
                "total_count": len(extracted_images),
                "removed_size": sum(img['size'] for img in extracted_images),
                "representative": [
                    {
                        "index": img['index'],
                        "alt": img['alt'],
                        "type": img['type'],
                        "size": img['size'],
                        "markdown": img['full']
                    }
                    for img in representative_imgs
                ]
            }
        }
    
    return analysis, intermediate_data

# ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€
def analyze_paper_with_llm(paper_info: Dict, markdown_content: str, json_metadata: Dict) -> PaperAnalysis:
    """ê¸°ì¡´ í•¨ìˆ˜ (ê°œì„  ë²„ì „ í˜¸ì¶œ)"""
    analysis, _ = analyze_paper_with_llm_improved(
        paper_info, markdown_content, json_metadata,
        use_hierarchical=True, use_overlap=True, return_intermediate=False
    )
    return analysis

# ===== Confluence =====
def _conf_get_page_by_title(title: str) -> Optional[Dict]:
    url = f"{CONFLUENCE_URL}/rest/api/content"
    params = {"title": title, "spaceKey": CONFLUENCE_SPACE_KEY, "expand": "version"}
    r = requests.get(url, params=params, auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=30)
    r.raise_for_status()
    res = r.json().get("results", [])
    return res[0] if res else None

def _conf_escape(s: str) -> str:
    return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def upload_to_confluence(analyses: List[PaperAnalysis], page_title: str):
    logger.info(f"Uploading to Confluence: {page_title}")
    body = [f"<h1>AI Paper Newsletter - {datetime.now().strftime('%Y-%m-%d')}</h1>",
            "<p>ì´ë²ˆ ì£¼ì˜ ì£¼ëª©í•  ë§Œí•œ AI ë…¼ë¬¸ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.</p>",
            '<ac:structured-macro ac:name="info"><ac:rich-text-body>',
            f"<p>ì´ {len(analyses)}í¸ì˜ ë…¼ë¬¸ì´ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.</p>",
            "</ac:rich-text-body></ac:structured-macro><hr/>"]
    for i, a in enumerate(analyses, 1):
        body.append(f"<h2>{i}. {_conf_escape(a.title)}</h2>")
        if a.authors:
            body.append(f"<p><strong>Authors:</strong> {_conf_escape(', '.join(a.authors[:8]))}</p>")
        if a.tags:
            body.append(f"<p><strong>Tags:</strong> {_conf_escape(', '.join(a.tags))}</p>")
        if a.abstract:
            body.append("<h3>Abstract</h3><p>" + _conf_escape(a.abstract) + "</p>")
        body.append("<h3>Analysis</h3>")
        body.append(a.summary)
        body.append(f"<p><em>Source:</em> s3://{a.source_file}</p>")
        body.append("<hr/>")
    content_html = "\n".join(body)

    create_url = f"{CONFLUENCE_URL}/rest/api/content"
    headers = {"Content-Type": "application/json"}
    try:
        existing = _conf_get_page_by_title(page_title)
        if existing:
            page_id = existing["id"]
            version = existing.get("version", {}).get("number", 1) + 1
            payload = {
                "id": page_id, "type": "page", "title": page_title,
                "space": {"key": CONFLUENCE_SPACE_KEY},
                "body": {"storage": {"value": content_html, "representation": "storage"}},
                "version": {"number": version},
            }
            r = requests.put(f"{create_url}/{page_id}", json=payload, headers=headers,
                             auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=60)
            r.raise_for_status()
            result = r.json()
        else:
            payload = {
                "type": "page", "title": page_title, "space": {"key": CONFLUENCE_SPACE_KEY},
                "body": {"storage": {"value": content_html, "representation": "storage"}},
            }
            r = requests.post(create_url, json=payload, headers=headers,
                              auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=60)
            r.raise_for_status()
            result = r.json()

        base = CONFLUENCE_URL.rstrip("/")
        webui = result.get("_links", {}).get("webui")
        tiny = result.get("_links", {}).get("tinyui")
        page_url = f"{base}{webui}" if webui else (f"{base}{tiny}" if tiny else f"{base}/pages/{result['id']}")
        logger.info(f"Confluence page: {page_url}")
        return {"success": True, "page_url": page_url, "page_id": result["id"]}
    except Exception as e:
        logger.exception("Confluence upload error")
        raise

# ===== Markdown builder =====
def derive_week_label(prefix: str) -> str:
    m = re.search(r"w(\d{1,2})", prefix or "", re.IGNORECASE)
    if m:
        return f"w{int(m.group(1))}"
    iso_year, iso_week, _ = datetime.utcnow().isocalendar()
    return f"w{iso_week}"

def build_markdown(
    analyses: List[PaperAnalysis], 
    papers_metadata: Optional[List[Dict]] = None,
    week_label: str = "", 
    prefix: str = ""
) -> Tuple[str, str]:
    """
    ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
    """
    if not week_label:
        week_label = derive_week_label(prefix)
    
    header = f"""# AI Paper Newsletter â€“ {week_label}
_Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_

Source prefix: `{prefix}`

---

"""
    
    # ì´ë¯¸ì§€ ë§¤í•‘ ìƒì„±
    image_map = {}
    if papers_metadata:
        for meta in papers_metadata:
            if meta.get('images_info') and meta.get('images_info', {}).get('representative_images'):
                s3_key = meta.get('s3_key', '')
                image_map[s3_key] = meta['images_info']
    
    parts = [header]
    
    for i, a in enumerate(analyses, 1):
        tags = f"**Tags:** {', '.join(a.tags)}" if a.tags else ""
        authors = f"**Authors:** {', '.join(a.authors[:8])}" if a.authors else ""
        
        abstract_block = ""
        if a.abstract and a.abstract.strip():
            abstract_block = f"\n**Abstract**\n\n> {a.abstract.strip()}\n\n"
        
        # â­ Summary JSON íŒŒì‹± ë° ê°œì¡°ì‹ ë³€í™˜
        summary_formatted = format_summary_as_markdown(a.summary)
        
        sec = f"""## {i}. {a.title}

{authors}
{tags}

{summary_formatted}

{abstract_block}"""
        
        # ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
        if a.source_file in image_map:
            img_info = image_map[a.source_file]
            rep_imgs = img_info.get('representative_images', [])
            
            if rep_imgs:
                rep_img = rep_imgs[0]
                paper_name = Path(a.source_file).stem
                img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{rep_img['type']}"
                
                sec += f"""### ğŸ“Š ëŒ€í‘œ ì´ë¯¸ì§€

**ì „ì²´ ì´ë¯¸ì§€:** {img_info['total_images']}ê°œ  
**ëŒ€í‘œ ì´ë¯¸ì§€:** Figure {rep_img['index'] + 1} ({rep_img['size_kb']:.1f}KB)

![Figure {rep_img['index'] + 1}](images/{img_filename})

"""
        
        sec += f"""**Source:** `s3://{a.source_file}`

---

"""
        parts.append(sec)
    
    md_content = "".join(parts)
    md_filename = f"{week_label}.md"
    return md_filename, md_content


def format_summary_as_markdown(summary: str) -> str:
    """
    Summary JSONì„ ë³´ê¸° ì¢‹ì€ Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        summary: JSON í˜•íƒœì˜ summary ë¬¸ìì—´
    
    Returns:
        í¬ë§·íŒ…ëœ Markdown ë¬¸ìì—´
    """
    try:
        # JSON ì¶”ì¶œ ì‹œë„
        summary_clean = summary.strip().replace('~', 'â€“')
        
        # JSON íŒŒì‹±
        json_match = re.search(r'\{[\s\S]*\}', summary_clean)
        if not json_match:
            # JSONì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return f"**Summary**\n\n{summary_clean}\n"
        
        data = json.loads(json_match.group(0))
        
        # Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
        lines = ["**Summary**\n"]
        
        # TL;DR
        if data.get('tldr'):
            lines.append(f"**ğŸ“Œ TL;DR**\n")
            lines.append(f"{data['tldr']}\n\n")
        
        # í•µì‹¬ ê¸°ì—¬
        if data.get('key_contributions'):
            lines.append(f"**ğŸ¯ í•µì‹¬ ê¸°ì—¬**\n")
            for contrib in data['key_contributions']:
                lines.append(f"- {contrib}\n")
            lines.append("\n")
        
        # ë°©ë²•ë¡ 
        if data.get('methodology'):
            lines.append(f"**ğŸ”¬ ë°©ë²•ë¡ **\n")
            lines.append(f"{data['methodology']}\n\n")
        
        # ê²°ê³¼
        if data.get('results'):
            lines.append(f"**ğŸ“Š ê²°ê³¼**\n")
            lines.append(f"{data['results']}\n\n")
        
        # ìƒˆë¡œìš´ ì 
        if data.get('novelty'):
            lines.append(f"**ğŸ’¡ ìƒˆë¡œìš´ ì **\n")
            lines.append(f"{data['novelty']}\n\n")
        
        # í•œê³„ì 
        if data.get('limitations'):
            lines.append(f"**âš ï¸ í•œê³„ì **\n")
            for limitation in data['limitations']:
                lines.append(f"- {limitation}\n")
            lines.append("\n")
        
        # Relevance Score
        if data.get('relevance_score'):
            score = data['relevance_score']
            stars = 'â­' * score
            lines.append(f"**ê´€ë ¨ì„± ì ìˆ˜:** {stars} ({score}/10)\n\n")
        
        return "".join(lines)
        
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        logger.warning(f"Failed to parse summary JSON: {e}")
        return f"**Summary**\n\n{summary.strip().replace('~', 'â€“')}\n"

# ===== Main Routes =====
@app.get("/")
async def root():
    return {
        "service": "AI Paper Newsletter Processor",
        "version": "2.0.0",
        "status": "running",
        "improvements": [
            "smart_hybrid_chunking (header-based or token-based)",
            "overlap_chunking",
            "hierarchical_summarization (position-based)"
        ],
        "available_endpoints": ["/health", "/list-s3-papers", "/process-s3-papers", "/debug/*"],
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat(), "mode": "S3-only (v2: hybrid chunking)"}

@app.get("/list-s3-papers")
async def list_s3_papers(bucket: Optional[str] = None, prefix: Optional[str] = None):
    bucket = bucket or S3_BUCKET
    prefix = prefix or S3_PAPERS_PREFIX
    try:
        papers = get_s3_papers(bucket, prefix)
        return {
            "bucket": bucket,
            "prefix": prefix,
            "papers_found": len(papers),
            "papers": [
                {"title": p["title"], "s3_key": p["s3_key"], "last_modified": p["last_modified"], "size_bytes": p.get("size_bytes", 0)}
                for p in papers
            ],
        }
    except Exception as e:
        logger.error(f"list_s3_papers error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-s3-papers")
async def process_s3_papers(request: S3PapersRequest):
    logger.info(f"Processing S3 papers (hierarchical={request.use_hierarchical}, overlap={request.use_overlap})")
    bucket = request.bucket or S3_BUCKET
    prefix = request.prefix or S3_PAPERS_PREFIX
    
    try:
        # 1ë‹¨ê³„: paper_list.txt í™•ì¸
        paper_urls = get_paper_list_from_s3(bucket, prefix)
        
        if paper_urls:
            # URL ê¸°ë°˜ ì²˜ë¦¬
            logger.info(f"Processing {len(paper_urls)} papers from paper_list.txt")
            papers = [
                {
                    "title": extract_title_from_url(url),
                    "url": url,
                    "source": "url",
                    "s3_key": f"{prefix}/{extract_title_from_url(url)}.pdf"  # ê°€ìƒ ê²½ë¡œ
                }
                for url in paper_urls
            ]
        else:
            # ê¸°ì¡´ PDF íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬
            logger.info("paper_list.txt not found, processing PDF files")
            papers = get_s3_papers(
                bucket=bucket,
                prefix=prefix,
                file_pattern=request.file_pattern or "*.pdf",
                process_subdirectories=request.process_subdirectories,
            )
        
        if not papers:
            return {
                "message": "No papers found",
                "papers_found": 0,
                "papers_processed": 0,
                "bucket": bucket,
                "prefix": prefix,
                "md_filename": None,
                "md_content": "",
            }

        def _process_one(p: Dict) -> Tuple[Optional[PaperAnalysis], Optional[Dict], Optional[str]]:
            """ë…¼ë¬¸ 1ê°œ ì²˜ë¦¬"""
            tmp = None
            try:
                # URL ê¸°ë°˜ vs íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬
                if p.get("source") == "url":
                    # URLì—ì„œ ì§ì ‘ íŒŒì‹±
                    md, meta = parse_pdf_with_docpamin_url(p["url"], p["title"])
                    
                    actual_title = meta.get('extracted_title', p["title"])
                    info = {
                        "title": actual_title,
                        "authors": [], 
                        "abstract": "", 
                        "s3_key": p["s3_key"]
                    }
                else:
                    # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ íŒŒì‹±
                    tmp = download_pdf_from_s3(p["s3_key"], p["s3_bucket"])
                    md, meta = parse_pdf_with_docpamin(tmp)
                    
                    actual_title = extract_title_from_markdown(md) if md else p["title"]
                    meta['extracted_title'] = actual_title
                    
                    info = {
                        "title": actual_title,
                        "authors": [], 
                        "abstract": "", 
                        "s3_key": p["s3_key"]
                    }
                
                a, _ = analyze_paper_with_llm_improved(
                    info, md, meta,
                    use_hierarchical=request.use_hierarchical,
                    use_overlap=request.use_overlap,
                    return_intermediate=False
                )
                a.source_file = p["s3_key"]
                return a, meta, None
                
            except Exception as e:
                return None, None, f"{p.get('title', 'unknown')}: {e}"
            finally:
                if tmp and os.path.exists(tmp):
                    try: os.unlink(tmp)
                    except Exception: pass

        analyses: List[PaperAnalysis] = []
        papers_metadata: List[Dict] = []
        errors: List[str] = []
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futs = [ex.submit(_process_one, p) for p in papers]
            for fut in as_completed(futs):
                a, meta, err = fut.result()
                if a: 
                    analyses.append(a)
                    if meta and meta.get('images_info'):
                        papers_metadata.append({
                            's3_key': a.source_file,
                            'title': a.title,
                            'images_info': meta['images_info']
                        })
                if err: 
                    errors.append(err)

        week_label = request.week_label or derive_week_label(prefix)
        md_filename, md_content = build_markdown(analyses, papers_metadata, week_label, prefix)

        confluence_result = None
        if request.upload_confluence and analyses:
            page_title = f"AI Paper Newsletter - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            confluence_result = upload_to_confluence(analyses, page_title)

        return {
            "message": "Processed",
            "papers_found": len(papers),
            "papers_processed": len(analyses),
            "errors": errors,
            "bucket": bucket,
            "prefix": prefix,
            "week_label": week_label,
            "md_filename": md_filename,
            "md_content": md_content,
            "papers_metadata": papers_metadata,
            "source": "url_list" if paper_urls else "pdf_files",
            "confluence_url": (confluence_result or {}).get("page_url"),
            "improvements_used": {
                "hierarchical": request.use_hierarchical,
                "overlap": request.use_overlap
            }
        }
    except Exception as e:
        logger.exception("process_s3_papers error")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch-process")
async def batch_process_papers(request: BatchProcessRequest):
    logger.info("Batch processing papers")
    bucket = request.bucket or S3_BUCKET
    prefix = request.prefix or S3_PAPERS_PREFIX
    try:
        papers = get_s3_papers(bucket, prefix)
        if not papers:
            return {"message": "No papers found in S3", "papers_processed": 0}

        analyses: List[PaperAnalysis] = []
        for p in papers:
            tmp = None
            try:
                tmp = download_pdf_from_s3(p["s3_key"], p["s3_bucket"])
                md, meta = parse_pdf_with_docpamin(tmp)
                info = {"title": p["title"], "authors": [], "abstract": "", "s3_key": p["s3_key"]}
                a = analyze_paper_with_llm(info, md, meta)
                a.source_file = p["s3_key"]
                a.tags = request.tags
                analyses.append(a)
            except Exception as e:
                logger.error(f"Error processing: {e}")
            finally:
                if tmp and os.path.exists(tmp):
                    try: os.unlink(tmp)
                    except Exception: pass

        page_title = request.confluence_page_title or f"AI Paper Review - {datetime.now().strftime('%Y-%m-%d')}"
        confluence_result = upload_to_confluence(analyses, page_title)
        return {
            "message": "Successfully batch processed papers",
            "papers_found": len(papers),
            "papers_processed": len(analyses),
            "confluence_url": confluence_result.get("page_url"),
        }
    except Exception as e:
        logger.exception("batch_process error")
        raise HTTPException(status_code=500, detail=str(e))

# ===== Debug Endpoints =====

@app.post("/debug/parse-file")
async def debug_parse_file(
    file: UploadFile = File(...),
    include_markdown: bool = Form(False),
    markdown_max_chars: int = Form(5000),
):
    """ë¡œì»¬ PDF íŒŒì¼ ì—…ë¡œë“œ â†’ Docpamin íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    suffix = Path(file.filename).suffix or ".pdf"
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        content = await file.read()
        tmp.write(content)
        tmp.close()

        md, meta = parse_pdf_with_docpamin(tmp.name)
        resp = {
            "filename": file.filename,
            "md_len": len(md),
            "md_preview": _preview(md, markdown_max_chars),
            "metadata": meta,
            "images_info": meta.get("images_info"),
            # ğŸ”„  Backwards compatibility: older scripts expect `json_metadata`
            #     while newer ones read `metadata`.
            "json_metadata": meta,
        }
        if include_markdown:
            resp["markdown"] = md
        return resp
    finally:
        try:
            if os.path.exists(tmp.name):
                os.unlink(tmp.name)
        except Exception:
            pass

@app.post("/debug/parse-s3")
async def debug_parse_s3(req: DebugParseS3Request):
    """S3 íŠ¹ì • key â†’ Docpamin íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    bucket = req.bucket or S3_BUCKET
    if not bucket:
        raise HTTPException(status_code=400, detail="Bucket is required")
    tmp = None
    try:
        tmp = download_pdf_from_s3(s3_key=req.key, s3_bucket=bucket)
        md, meta = parse_pdf_with_docpamin(tmp)
        resp = {
            "bucket": bucket,
            "key": req.key,
            "md_len": len(md),
            "md_preview": _preview(md, req.markdown_max_chars),
            "metadata": meta,
            "images_info": meta.get("images_info"),
            # ğŸ”„  Maintain both keys so workflows consuming either continue working.
            "json_metadata": meta,
        }
        if req.include_markdown:
            resp["markdown"] = md
        return resp
    finally:
        try:
            if tmp and os.path.exists(tmp):
                os.unlink(tmp)
        except Exception:
            pass

@app.post("/debug/summarize-markdown")
async def debug_summarize_markdown(req: DebugSummarizeMarkdownRequest):
    """
    Markdown í…ìŠ¤íŠ¸ â†’ ì²­í¬/ìµœì¢… ìš”ì•½ í…ŒìŠ¤íŠ¸ (ê°œì„  ë²„ì „ v2)
    - use_hierarchical: ê³„ì¸µì  ìš”ì•½ ì‚¬ìš©
    - use_overlap: overlap chunking ì‚¬ìš©
    - show_intermediate_steps: ì¤‘ê°„ ë‹¨ê³„ ì¶œë ¥
    """
    if not req.markdown or not req.markdown.strip():
        raise HTTPException(status_code=400, detail="markdown is empty")

    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (í•˜ì´ë¸Œë¦¬ë“œ)
    chunks, chunking_method = smart_chunk_hybrid(req.markdown, min_headers=5)
    
    chunk_summaries = {}
    if req.include_section_summaries:
        prev_summary = ""
        for chunk_key, chunk_text in chunks.items():
            if not chunk_text.strip() or len(chunk_text.strip()) < 100:
                continue
            chunk_summaries[chunk_key] = summarize_chunk_with_overlap(
                chunk_key, chunk_text, req.title, 
                use_overlap=req.use_overlap,
                prev_summary=prev_summary if req.use_overlap else ""
            )
            prev_summary = chunk_summaries[chunk_key]

    final_analysis = None
    intermediate_data = None
    if req.include_final_analysis:
        paper_info = {"title": req.title, "authors": [], "abstract": "", "s3_key": ""}
        final, intermediate = analyze_paper_with_llm_improved(
            paper_info, req.markdown, {},
            use_hierarchical=req.use_hierarchical,
            use_overlap=req.use_overlap,
            return_intermediate=req.show_intermediate_steps
        )
        final_analysis = {
            "title": final.title,
            "authors": final.authors,
            "abstract": final.abstract,
            "summary": final.summary,
            "key_contributions": final.key_contributions,
            "methodology": final.methodology,
            "results": final.results,
            "relevance_score": final.relevance_score,
            "tags": final.tags,
        }
        intermediate_data = intermediate

    return {
        "title": req.title,
        "chunking_method": chunking_method,
        "chunks_detected": list(chunks.keys()),
        "chunk_summaries": chunk_summaries if req.include_section_summaries else {},
        "final_analysis": final_analysis,
        "intermediate_steps": intermediate_data if req.show_intermediate_steps else None,
        "markdown_preview": _preview(req.markdown, req.return_markdown_preview_chars),
        "improvements_used": {
            "hierarchical": req.use_hierarchical,
            "overlap": req.use_overlap
        }
    }

@app.post("/debug/summarize-sections")
async def debug_summarize_sections(req: DebugSummarizeSectionsRequest):
    """ì„¹ì…˜/ì²­í¬ dict â†’ ìš”ì•½ í…ŒìŠ¤íŠ¸ (ê°œì„  ë²„ì „)"""
    if not req.sections:
        raise HTTPException(status_code=400, detail="sections is empty")

    targets = req.only_sections or list(req.sections.keys())
    out: Dict[str, str] = {}
    prev_summary = ""
    
    for sec in targets:
        text = req.sections.get(sec, "")
        if not text.strip():
            continue
        out[sec] = summarize_chunk_with_overlap(
            sec, text, req.title, 
            use_overlap=req.use_overlap,
            prev_summary=prev_summary if req.use_overlap else ""
        )
        prev_summary = out[sec]

    return {
        "title": req.title,
        "summarized_sections": list(out.keys()),
        "summaries": out,
        "improvements_used": {
            "overlap": req.use_overlap
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("paper_processor:app", host="0.0.0.0", port=7070, reload=False, workers=2)

    