#!/usr/bin/env python3

from fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Tuple, Iterable
from datetime import datetime
from pathlib import Path
import boto3
from botocore.config import Config
import tempfile
import logging
import json
import time
import requests
import zipfile
import io
import re
import fnmatch
import math
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import textwrap
import base64
import hashlib
from dotenv import load_dotenv, dotenv_values

load_dotenv(override=True)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


app = FastAPI(
    title="AI Paper Newsletter Processor",
    description="Processes AI papers from S3, parses with Docpamin, summarizes via LLM. v2: Smart Hybrid Chunking (header-based or token-based)",
    version="2.0.0",
    docs_url="/docs",
    redoc_url=None,
    openapi_url="/openapi.json",
    root_path="/proxy/7070",
)

# ===== Configuration (env) =====
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
S3_BUCKET = os.getenv("S3_BUCKET_NAME")
S3_PAPERS_PREFIX = os.getenv("S3_PAPERS_PREFIX", "papers/")

DOCPAMIN_API_KEY = os.getenv("DOCPAMIN_API_KEY")
DOCPAMIN_BASE_URL = os.getenv("DOCPAMIN_BASE_URL", "https://docpamin.superaip.samsungds.net/api/v1")
DOCPAMIN_CRT_FILE = os.getenv("DOCPAMIN_CRT_FILE", "/etc/ssl/certs/ca-certificates.crt")

LLM_API_KEY = os.getenv("LLM_API_KEY")
LLM_BASE_URL = os.getenv("LLM_BASE_URL")
LLM_MODEL = os.getenv("LLM_MODEL", "openai/gpt-oss-120b")
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "30000"))

CONFLUENCE_URL = os.getenv("CONFLUENCE_URL")
CONFLUENCE_EMAIL = os.getenv("CONFLUENCE_EMAIL")
CONFLUENCE_API_TOKEN = os.getenv("CONFLUENCE_API_TOKEN")
CONFLUENCE_SPACE_KEY = os.getenv("CONFLUENCE_SPACE_KEY")

MAX_WORKERS = int(os.getenv("MAX_WORKERS", "8"))


# ===== boto3 client =====
boto_config = Config(
    retries={"max_attempts": 5, "mode": "standard"},
    connect_timeout=10,
    read_timeout=60,
)
s3_client = boto3.client(
    "s3",
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY,
    config=boto_config,
)

# ===== Models =====
class S3PapersRequest(BaseModel):
    bucket: Optional[str] = None
    prefix: Optional[str] = None
    file_pattern: Optional[str] = "*.pdf"
    process_subdirectories: bool = True
    week_label: Optional[str] = None
    upload_confluence: Optional[bool] = False
    # ê°œì„  ì˜µì…˜
    use_hierarchical: bool = True  # ê³„ì¸µì  ìš”ì•½ ì‚¬ìš©
    use_overlap: bool = True  # overlap chunking ì‚¬ìš©

class BatchProcessRequest(BaseModel):
    bucket: Optional[str] = None
    prefix: Optional[str] = None
    confluence_page_title: Optional[str] = None
    tags: List[str] = ["AI", "Research"]

class PaperAnalysis(BaseModel):
    title: str
    authors: List[str]
    abstract: str
    summary: str
    key_contributions: List[str]
    methodology: str
    results: str
    relevance_score: int
    tags: List[str]
    source_file: str

class DebugParseS3Request(BaseModel):
    bucket: Optional[str] = None
    key: str
    include_markdown: bool = False
    markdown_max_chars: int = 5000

class DebugSummarizeMarkdownRequest(BaseModel):
    title: str = "Untitled Paper"
    markdown: str
    include_section_summaries: bool = True
    include_final_analysis: bool = True
    return_markdown_preview_chars: int = 0
    # ê°œì„  ì˜µì…˜
    use_hierarchical: bool = True
    use_overlap: bool = True
    show_intermediate_steps: bool = False  # ì¤‘ê°„ ë‹¨ê³„ ì¶œë ¥

class DebugSummarizeSectionsRequest(BaseModel):
    title: str = "Untitled Paper"
    sections: Dict[str, str] = Field(default_factory=dict)
    only_sections: Optional[List[str]] = None
    # ê°œì„  ì˜µì…˜
    use_overlap: bool = True

# ===== Utilities =====
def _iter_s3_objects(bucket: str, prefix: str) -> Iterable[Dict]:
    paginator = s3_client.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            yield obj

def _preview(text: str, limit: int) -> str:
    if not text or limit <= 0:
        return ""
    if len(text) <= limit:
        return text
    return text[:limit] + "\n...\n(truncated)"

def get_paper_list_from_s3(bucket: str, prefix: str) -> Optional[List[str]]:
    """
    S3ì—ì„œ paper_list.txt íŒŒì¼ì„ ì½ì–´ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    Args:
        bucket: S3 ë²„í‚·ëª…
        prefix: S3 prefix (ì˜ˆ: kai_papers/w43)
    
    Returns:
        URL ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” None (íŒŒì¼ ì—†ìŒ)
    """
    paper_list_key = f"{prefix.rstrip('/')}/paper_list.txt"
    
    try:
        logger.info(f"Checking for paper list: s3://{bucket}/{paper_list_key}")
        
        response = s3_client.get_object(Bucket=bucket, Key=paper_list_key)
        content = response['Body'].read().decode('utf-8')
        
        # URL íŒŒì‹± (ë¹ˆ ì¤„, ì£¼ì„ ì œì™¸)
        urls = []
        for line in content.splitlines():
            line = line.strip()
            # ë¹ˆ ì¤„ì´ë‚˜ # ì£¼ì„ ì œì™¸
            if not line or line.startswith('#'):
                continue
            # URL í˜•ì‹ í™•ì¸
            if line.startswith('http'):
                urls.append(line)
            else:
                logger.warning(f"Invalid URL format: {line}")
        
        logger.info(f"Found {len(urls)} URLs in paper_list.txt")
        return urls if urls else None
        
    except s3_client.exceptions.NoSuchKey:
        logger.info(f"paper_list.txt not found: {paper_list_key}")
        return None
    except Exception as e:
        logger.error(f"Error reading paper_list.txt: {e}")
        return None

def get_s3_papers(
    bucket: str,
    prefix: str,
    file_pattern: str = "*.pdf",
    process_subdirectories: bool = True,
    min_size_bytes: int = 1024,
    max_size_bytes: int = 1024 * 1024 * 100,
) -> List[Dict]:
    logger.info(f"Fetching papers from S3: s3://{bucket}/{prefix}")
    papers = []
    for obj in _iter_s3_objects(bucket, prefix):
        key = obj["Key"]
        rel = key[len(prefix):].lstrip("/") if key.startswith(prefix) else key
        if not fnmatch.fnmatch(rel, file_pattern):
            continue
        if not process_subdirectories and "/" in rel:
            continue
        size = obj.get("Size", 0)
        if size < min_size_bytes or size > max_size_bytes:
            continue
        papers.append({
            "title": Path(key).stem.replace("_", " ").replace("-", " "),
            "s3_key": key,
            "s3_bucket": bucket,
            "last_modified": obj["LastModified"].isoformat(),
            "size_bytes": size,
            "source": "s3",
        })
    logger.info(f"Found {len(papers)} papers in S3")
    return papers

def download_pdf_from_s3(s3_key: str, s3_bucket: str) -> str:
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
    try:
        logger.info(f"Downloading PDF: s3://{s3_bucket}/{s3_key}")
        s3_client.download_fileobj(s3_bucket, s3_key, tmp)
        tmp.close()
        return tmp.name
    except Exception as e:
        tmp.close()
        if os.path.exists(tmp.name):
            try: os.unlink(tmp.name)
            except Exception: pass
        raise Exception(f"Failed to download PDF: {e}")

# ===== Docpamin =====
def get_docpamin_cache_key(source_identifier: str) -> str:
    """
    ìºì‹œ í‚¤ ìƒì„± (arXiv ID ë˜ëŠ” íŒŒì¼ í•´ì‹œ)
    
    Args:
        source_identifier: arXiv ID (ì˜ˆ: 2510.11701) ë˜ëŠ” S3 key
    
    Returns:
        ìºì‹œ í‚¤ (ì˜ˆ: "2510.11701" ë˜ëŠ” MD5 í•´ì‹œ)
    """
    # arXiv ID ì¶”ì¶œ
    arxiv_match = re.search(r'(\d{4}\.\d{5})', source_identifier)
    if arxiv_match:
        return arxiv_match.group(1)
    
    # ì¼ë°˜ íŒŒì¼ëª…ì—ì„œ í•´ì‹œ ìƒì„±
    return hashlib.md5(source_identifier.encode()).hexdigest()[:16]

def save_docpamin_cache_to_s3(
    bucket: str,
    prefix: str,
    cache_key: str,
    markdown: str,
    metadata: Dict
):
    """
    Docpamin ê²°ê³¼ë¥¼ S3ì— ìºì‹±
    
    Args:
        bucket: S3 ë²„í‚·
        prefix: S3 prefix (ì˜ˆ: kai_papers/w44)
        cache_key: ìºì‹œ í‚¤
        markdown: íŒŒì‹±ëœ markdown
        metadata: JSON metadata
    """
    try:
        cache_prefix = f"{prefix.rstrip('/')}/cache"
        
        # Markdown ì €ì¥
        md_key = f"{cache_prefix}/{cache_key}.md"
        s3_client.put_object(
            Bucket=bucket,
            Key=md_key,
            Body=markdown.encode('utf-8'),
            ContentType='text/markdown'
        )
        logger.info(f"Saved markdown cache: s3://{bucket}/{md_key}")
        
        # Metadata ì €ì¥
        json_key = f"{cache_prefix}/{cache_key}.json"
        s3_client.put_object(
            Bucket=bucket,
            Key=json_key,
            Body=json.dumps(metadata, ensure_ascii=False).encode('utf-8'),
            ContentType='application/json'
        )
        logger.info(f"Saved metadata cache: s3://{bucket}/{json_key}")
        
        return True
        
    except Exception as e:
        logger.error(f"Failed to save Docpamin cache: {e}")
        return False


def load_docpamin_cache_from_s3(
    bucket: str,
    prefix: str,
    cache_key: str
) -> Tuple[Optional[str], Optional[Dict]]:
    """
    S3ì—ì„œ Docpamin ìºì‹œ ë¡œë“œ
    
    Returns:
        (markdown, metadata) ë˜ëŠ” (None, None)
    """
    try:
        cache_prefix = f"{prefix.rstrip('/')}/cache"
        
        # Markdown ë¡œë“œ
        md_key = f"{cache_prefix}/{cache_key}.md"
        logger.info(f"Checking cache: s3://{bucket}/{md_key}")
        
        md_response = s3_client.get_object(Bucket=bucket, Key=md_key)
        markdown = md_response['Body'].read().decode('utf-8')
        
        # Metadata ë¡œë“œ
        json_key = f"{cache_prefix}/{cache_key}.json"
        json_response = s3_client.get_object(Bucket=bucket, Key=json_key)
        metadata = json.loads(json_response['Body'].read().decode('utf-8'))
        
        logger.info(f"âœ… Loaded from cache: {cache_key} (md_len={len(markdown)})")
        return markdown, metadata
        
    except s3_client.exceptions.NoSuchKey:
        logger.info(f"Cache not found: {cache_key}")
        return None, None
    except Exception as e:
        logger.error(f"Failed to load cache: {e}")
        return None, None



def parse_pdf_with_docpamin(pdf_path: str) -> Tuple[str, Dict]:
    logger.info(f"Parsing via Docpamin: {pdf_path}")
    headers = {"Authorization": f"Bearer {DOCPAMIN_API_KEY}"}
    session = requests.Session()
    session.headers.update(headers)
    REQ_TIMEOUT = 30

    try:
        with open(pdf_path, "rb") as f:
            files = {"file": f}
            data = {
                "alarm_options": json.dumps({"enabled": False}),
                "workflow_options": json.dumps({
                    "workflow": "dp-o1",
                    "image_export_mode": "embedded"
                }),
            }
            r = session.post(f"{DOCPAMIN_BASE_URL}/tasks", files=files, data=data,
                             verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
        r.raise_for_status()
        task_id = r.json().get("task_id")
        if not task_id:
            raise Exception("Docpamin: no task_id returned")

        logger.info(f"Docpamin task: {task_id}")
        max_wait, waited, backoff = 600, 0, 2
        while waited < max_wait:
            s = session.get(f"{DOCPAMIN_BASE_URL}/tasks/{task_id}",
                            verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
            s.raise_for_status()
            status = s.json().get("status")
            if status == "DONE":
                break
            if status in {"FAILED", "ERROR"}:
                raise Exception(f"Docpamin task failed: {status}")
            time.sleep(backoff)
            waited += backoff
            backoff = min(backoff * 1.5, 10)
        if waited >= max_wait:
            raise Exception("Docpamin timeout")

        opts = {"task_ids": [task_id], "output_types": ["markdown", "json"]}
        e = session.post(f"{DOCPAMIN_BASE_URL}/tasks/export", json=opts,
                         verify=DOCPAMIN_CRT_FILE, timeout=REQ_TIMEOUT)
        e.raise_for_status()

        md, meta = "", {}
        with zipfile.ZipFile(io.BytesIO(e.content)) as zf:
            for fn in zf.namelist():
                with zf.open(fn) as fh:
                    if fn.endswith(".md"):
                        s = fh.read().decode("utf-8", errors="ignore")
                        if len(s) > len(md):
                            md = s
                    elif fn.endswith(".json"):
                        try:
                            meta = json.loads(fh.read().decode("utf-8", errors="ignore"))
                        except Exception:
                            pass
        if not md:
            raise Exception("Docpamin: no markdown in export")

        paper_title = extract_title_from_markdown(md)
        meta['extracted_title'] = paper_title
        logger.info(f"Docpamin parsed OK (md_len={len(md)})")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬: base64 ì œê±°, ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ
        md_cleaned, extracted_images = process_markdown_images(
            md, 
            remove_for_llm=True,  # LLM ì…ë ¥ìš©ìœ¼ë¡œ base64 ì œê±°
            keep_representative=1
        )
        
        # ë©”íƒ€ë°ì´í„°ì— ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
        if extracted_images:
            representative = select_representative_images(
                extracted_images, 
                max_count=1,
                paper_title=paper_title
            )
            meta['images_info'] = {
                'total_images': len(extracted_images),
                'representative_images': representative
            }
            logger.info(f"Image preprocessing: {len(extracted_images)} images, "
                       f"markdown size reduced from {len(md)} to {len(md_cleaned)} chars")
        
        return md_cleaned, meta
    except Exception as e:
        logger.error(f"Docpamin error: {e}")
        raise

def parse_pdf_with_docpamin_url(pdf_url: str, arxiv_id: str = "") -> Tuple[str, Dict]:
    """
    URLë¡œ PDF íŒŒì‹± (JSONë§Œ ì‚¬ìš©!)
    """
    cache_key = get_docpamin_cache_key(arxiv_id or pdf_url)
    bucket = S3_BUCKET
    prefix = S3_PAPERS_PREFIX
    
    cached_md, cached_meta = load_docpamin_cache_from_s3(bucket, prefix, cache_key)
    
    if cached_md and cached_meta:
        logger.info(f"ğŸ“¦ Using cached Docpamin for {cache_key}")
        
        paper_title = extract_title_from_markdown(cached_md)
        cached_meta['extracted_title'] = paper_title
        
        # â­ ì´ë¯¸ì§€ë§Œ ì¶”ì¶œ (caption ì—†ìŒ)
        md_cleaned, extracted_images = process_markdown_images(
            cached_md,
            remove_for_llm=True
        )
        
        if extracted_images and cached_meta:
            # â­ JSONì—ì„œë§Œ caption ë§¤ì¹­!
            figure_pairs = extract_figure_pairs_from_json(cached_meta)
            
            if figure_pairs:
                extracted_images = match_images_with_figure_pairs(
                    extracted_images,
                    figure_pairs
                )
            
            # â­ Caption ìˆëŠ” ì´ë¯¸ì§€ë§Œ ì„ íƒ
            images_with_caption = [
                img for img in extracted_images 
                if img.get('caption') and is_valid_caption(img.get('caption'))
            ]
            
            logger.info(f"Images with valid captions: {len(images_with_caption)}/{len(extracted_images)}")
            
            if images_with_caption:
                representative = select_representative_images(
                    images_with_caption,
                    max_count=1,
                    paper_title=paper_title
                )
                
                cached_meta['images_info'] = {
                    'total_images': len(extracted_images),
                    'images_with_caption': len(images_with_caption),
                    'representative_images': representative
                }
        
        return md_cleaned, cached_meta
    
    # ìºì‹œ ì—†ìŒ â†’ Docpamin íŒŒì‹±
    logger.info(f"Parsing via Docpamin (URL): {pdf_url}")
    headers = {"Authorization": f"Bearer {DOCPAMIN_API_KEY}"}
    session = requests.Session()
    session.headers.update(headers)
    REQ_TIMEOUT = 30

    try:
        data = {
            "file_url": pdf_url,
            "alarm_options": json.dumps({"enabled": False}),
            "workflow_options": json.dumps({
                "workflow": "dp-o1",
                "image_export_mode": "embedded"
            }),
        }
        
        r = session.post(
            f"{DOCPAMIN_BASE_URL}/tasks",
            data=data,
            verify=DOCPAMIN_CRT_FILE,
            timeout=REQ_TIMEOUT
        )
        r.raise_for_status()
        task_id = r.json().get("task_id")
        if not task_id:
            raise Exception("Docpamin: no task_id returned")

        logger.info(f"Docpamin task: {task_id}")
        
        # ìƒíƒœ í´ë§
        max_wait, waited, backoff = 600, 0, 2
        while waited < max_wait:
            s = session.get(
                f"{DOCPAMIN_BASE_URL}/tasks/{task_id}",
                verify=DOCPAMIN_CRT_FILE,
                timeout=REQ_TIMEOUT
            )
            s.raise_for_status()
            status = s.json().get("status")
            if status == "DONE":
                break
            if status in {"FAILED", "ERROR"}:
                raise Exception(f"Docpamin task failed: {status}")
            time.sleep(backoff)
            waited += backoff
            backoff = min(backoff * 1.5, 10)
        
        if waited >= max_wait:
            raise Exception("Docpamin timeout")

        # Export
        opts = {"task_ids": [task_id], "output_types": ["markdown", "json"]}
        e = session.post(
            f"{DOCPAMIN_BASE_URL}/tasks/export",
            json=opts,
            verify=DOCPAMIN_CRT_FILE,
            timeout=REQ_TIMEOUT
        )
        e.raise_for_status()

        md, meta = "", {}
        with zipfile.ZipFile(io.BytesIO(e.content)) as zf:
            for fn in zf.namelist():
                with zf.open(fn) as fh:
                    if fn.endswith(".md"):
                        s = fh.read().decode("utf-8", errors="ignore")
                        if len(s) > len(md):
                            md = s
                    elif fn.endswith(".json"):
                        try:
                            meta = json.loads(fh.read().decode("utf-8", errors="ignore"))
                        except Exception:
                            pass
        
        if not md:
            raise Exception("No markdown in export")
        
        # ìºì‹œ ì €ì¥
        save_docpamin_cache_to_s3(bucket, prefix, cache_key, md, meta)
        
        paper_title = extract_title_from_markdown(md)
        meta['extracted_title'] = paper_title
        meta['from_cache'] = False
        
        logger.info(f"Docpamin parsed (md_len={len(md)}, title={paper_title})")
        
        md_cleaned, extracted_images = process_markdown_images(
            md,
            remove_for_llm=True,
            keep_representative=1
        )
        
        if extracted_images:
            # â­ JSON metadata ì‚¬ìš©
            extracted_images = match_images_with_captions_from_json(
                extracted_images,
                meta
            )
            
            representative = select_representative_images(
                extracted_images,
                max_count=1,
                paper_title=paper_title
            )
            
            meta['images_info'] = {
                'total_images': len(extracted_images),
                'representative_images': representative
            }
        
        return md_cleaned, meta
        
    except Exception as e:
        logger.error(f"Docpamin error: {e}")
        raise

def extract_title_from_url(url: str) -> str:
    """
    URLì—ì„œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
    
    Args:
        url: arXiv URL (ì˜ˆ: https://arxiv.org/pdf/2312.12391.pdf)
    
    Returns:
        ì œëª© (arXiv ID)
    """
    # arXiv ID ì¶”ì¶œ
    match = re.search(r'(\d{4}\.\d{5})', url)
    if match:
        return match.group(1)
    
    # ì¼ë°˜ URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
    from urllib.parse import urlparse
    path = urlparse(url).path
    return Path(path).stem or "unknown"

def extract_title_from_markdown(markdown: str) -> str:
    """
    Docpamin markdownì—ì„œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
    
    Args:
        markdown: Docpaminì´ ë°˜í™˜í•œ markdown
    
    Returns:
        ë…¼ë¬¸ ì œëª© (ì²« ë²ˆì§¸ ## í—¤ë”©)
    """
    try:
        # ì²« ë²ˆì§¸ ## í—¤ë”© ì°¾ê¸°
        lines = markdown.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('##') and not line.startswith('###'):
                # ## ì œê±°í•˜ê³  ì œëª©ë§Œ ì¶”ì¶œ
                title = line.lstrip('#').strip()
                if title:
                    logger.info(f"Extracted title from markdown: {title}")
                    return title
        
        logger.warning("No title found in markdown")
        return "Unknown"
        
    except Exception as e:
        logger.error(f"Error extracting title: {e}")
        return "Unknown"


# ===== Image Preprocessing Functions =====

def remove_base64_images(markdown: str, replacement: str = "[Image]") -> Tuple[str, int]:
    """
    Base64 ì´ë¯¸ì§€ë¥¼ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ëŒ€ì²´
    
    Returns:
        (cleaned_markdown, num_removed)
    """
    pattern = r'!\[([^\]]*)\]\(data:image/[^;]+;base64,[A-Za-z0-9+/=]+\)'
    cleaned, count = re.subn(pattern, replacement, markdown)
    if count > 0:
        logger.info(f"Removed {count} base64 images from markdown")
    return cleaned, count

def extract_base64_images(markdown: str) -> List[Dict]:
    """Markdownì—ì„œ base64 ì´ë¯¸ì§€ ì¶”ì¶œ"""
    pattern = r'!\[([^\]]*)\]\(data:image/([^;]+);base64,([A-Za-z0-9+/=]+)\)'
    images = []
    for match in re.finditer(pattern, markdown):
        base64_data = match.group(3)
        size_bytes = len(base64_data) * 3 // 4
        images.append({
            'full_match': match.group(0),
            'alt_text': match.group(1),
            'mime_type': match.group(2),
            'base64_data': base64_data,
            'size_kb': size_bytes / 1024,
            'position': match.start()
        })
    return images
    
def extract_figure_pairs_from_json(json_metadata: Dict) -> List[Dict]:
    """
    Docpamin JSONì—ì„œ PICTURE-CAPTION ìŒë§Œ ì¶”ì¶œ
    
    Returns:
        [{'figure_number': 1, 'caption': '...', 'base64_preview': '...'}, ...]
    """
    figure_pairs = []
    
    try:
        pages = json_metadata.get('pages', [])
        
        for page in pages:
            layout = page.get('layout', [])
            
            # â­ PICTURE ë‹¤ìŒì— CAPTIONì´ ì˜¤ëŠ”ì§€ í™•ì¸
            for i, block in enumerate(layout):
                if block.get('type') != 'PICTURE':
                    continue
                
                # ë‹¤ìŒ ë¸”ë¡ í™•ì¸
                if i + 1 >= len(layout):
                    continue
                
                next_block = layout[i + 1]
                
                # â­ ë‹¤ìŒ ë¸”ë¡ì´ CAPTIONì¸ì§€ í™•ì¸
                if next_block.get('type') != 'CAPTION':
                    logger.debug(f"PICTURE at id={block.get('id')} has no CAPTION (next: {next_block.get('type')})")
                    continue
                
                # Caption ì¶”ì¶œ
                caption_content = next_block.get('content', '').strip()
                
                if not caption_content:
                    continue
                
                # Figure ë²ˆí˜¸ ì¶”ì¶œ
                fig_match = re.search(
                    r'Figure[~\s]+(\d+)[:\.]?\s*(.+?)$',
                    caption_content,
                    re.IGNORECASE
                )
                
                if not fig_match:
                    continue
                
                fig_num = int(fig_match.group(1))
                caption_text = fig_match.group(2).strip()
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if not is_valid_caption(caption_text):
                    logger.debug(f"Invalid caption for Figure {fig_num}")
                    continue
                
                # Base64 ë¯¸ë¦¬ë³´ê¸° (ë§¤ì¹­ìš©)
                picture_content = block.get('content', '')
                base64_match = re.search(r'base64,([A-Za-z0-9+/=]{50,100})', picture_content)
                base64_preview = base64_match.group(1) if base64_match else ''
                
                figure_pairs.append({
                    'figure_number': fig_num,
                    'caption': caption_text,
                    'base64_preview': base64_preview,
                    'page_no': page.get('page_no'),
                    'picture_id': block.get('id'),
                    'caption_id': next_block.get('id')
                })
                
                logger.info(f"ğŸ“· Figure {fig_num}: {caption_text[:60]}...")
        
        logger.info(f"Found {len(figure_pairs)} valid PICTURE-CAPTION pairs")
        
    except Exception as e:
        logger.error(f"Failed to extract figure pairs: {e}")
    
    return figure_pairs


def match_images_with_figure_pairs(
    images: List[Dict],
    figure_pairs: List[Dict]
) -> List[Dict]:
    """
    ì´ë¯¸ì§€ì™€ Figure ìŒ ë§¤ì¹­ (base64 ê¸°ë°˜)
    """
    if not figure_pairs:
        logger.warning("No figure pairs to match")
        return images
    
    matched_count = 0
    
    for img in images:
        img_base64 = img.get('base64_data', '')
        
        if not img_base64 or len(img_base64) < 100:
            continue
        
        # â­ Base64 ì•ë¶€ë¶„ìœ¼ë¡œ ë§¤ì¹­
        img_preview = img_base64[:100]
        
        for pair in figure_pairs:
            pair_preview = pair.get('base64_preview', '')
            
            # Base64ê°€ ë§¤ì¹­ë˜ë©´
            if pair_preview and pair_preview in img_preview:
                img['caption'] = pair['caption']
                img['figure_number'] = pair['figure_number']
                
                matched_count += 1
                logger.info(f"âœ… Image {img['index']} â†’ Figure {pair['figure_number']}: "
                           f"{pair['caption'][:60]}...")
                break
    
    logger.info(f"Matched {matched_count}/{len(images)} images with captions")
    
    return images


def select_representative_image(images: List[Dict], min_kb: float = 10, max_kb: float = 200) -> Optional[Dict]:
    """ëŒ€í‘œ ì´ë¯¸ì§€ ì„ ì • (í¬ê¸° + ìœ„ì¹˜ ê¸°ì¤€)"""
    if not images:
        return None
    candidates = [img for img in images if min_kb <= img['size_kb'] <= max_kb]
    if not candidates:
        candidates = sorted(images, key=lambda x: abs(x['size_kb'] - (min_kb + max_kb) / 2))[:3]
    return min(candidates, key=lambda x: x['position']) if candidates else None


# ===== Image Processing =====
def process_markdown_images(
    markdown: str, 
    remove_for_llm: bool = True,
    keep_representative: int = 1
) -> Tuple[str, List[Dict]]:
    """
    Markdownì—ì„œ ì´ë¯¸ì§€ë§Œ ì¶”ì¶œ (Caption ë§¤ì¹­ ì—†ìŒ!)
    """
    pattern = r'!\[(.*?)\]\(data:image/([^;]+);base64,([A-Za-z0-9+/=]+)\)'
    
    images = []
    
    def extract_image(match):
        alt_text = match.group(1)
        img_type = match.group(2)
        base64_data = match.group(3)
        
        images.append({
            'index': len(images),
            'alt': alt_text.strip(),
            'type': img_type,
            'size': len(base64_data),
            'size_kb': len(base64_data) * 3 / 4 / 1024,
            'base64_data': base64_data,
            'full': match.group(0)
        })
        
        if remove_for_llm:
            return f"\n[Image {len(images)}]\n"
        else:
            return match.group(0)
    
    processed_md = re.sub(pattern, extract_image, markdown)
    
    if images:
        logger.info(f"Extracted {len(images)} images from markdown")
    
    # âš ï¸ Caption ë§¤ì¹­ì€ ì—¬ê¸°ì„œ í•˜ì§€ ì•ŠìŒ!
    # parse_pdf_with_docpamin_urlì—ì„œ JSON ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­
    
    return processed_md, images

def is_valid_caption(caption: str) -> bool:
    """
    Caption ìœ íš¨ì„± ê²€ì‚¬ (base64, í•´ì‹œê°’ ë“± ì œê±°)
    """
    if not caption or len(caption) < 10:
        return False
    
    #  Base64 íŒ¨í„´ ê±°ë¶€
    base64_pattern = r'^[A-Za-z0-9+/=]{50,}$'
    if re.match(base64_pattern, caption):
        logger.debug(f"Rejected caption (base64): {caption[:50]}...")
        return False
    
    #  ë„ˆë¬´ ê¸´ ë‹¨ì–´ í•˜ë‚˜ë¡œë§Œ êµ¬ì„± (í•´ì‹œê°’)
    words = caption.split()
    if len(words) == 1 and len(words[0]) > 40:
        logger.debug(f"Rejected caption (hash): {caption[:50]}...")
        return False
    
    #  ì˜ë¯¸ìˆëŠ” ì˜ì–´ ë‹¨ì–´ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš°
    english_words = [w for w in words if re.match(r'^[a-zA-Z]+$', w) and len(w) > 2]
    if len(english_words) < 2:
        logger.debug(f"Rejected caption (no words): {caption[:50]}...")
        return False
    
    # ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ ê±°ë¶€
    if caption.startswith('![') or caption.startswith(']('):
        return False
    
    return True


def select_representative_image_with_llm(images: List[Dict], paper_title: str = "") -> Dict:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ëŒ€í‘œì ì¸ ì´ë¯¸ì§€ ì„ íƒ
    (ì‚¬ì „ í•„í„°ë§ ì—†ì´ LLM í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©)
    """
    if not images:
        return None
    
    if len(images) == 1:
        return images[0]
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ¯ select_representative_image_with_llm")
        logger.info(f"Total images: {len(images)}")
        
        #  Caption ìœ íš¨ì„± ê²€ì‚¬ë§Œ ìˆ˜í–‰
        images_with_valid_caption = []
        for img in images:
            caption = img.get('caption', '')
            
            if is_valid_caption(caption):
                images_with_valid_caption.append(img)
                logger.debug(f"  âœ… Image {img['index']}: {caption[:50]}...")
            else:
                logger.info(f"  âŒ Skipped image {img['index']}: Invalid caption")
        
        logger.info(f"Valid captions: {len(images_with_valid_caption)}/{len(images)}")
        logger.info("=" * 60)
        
        if not images_with_valid_caption:
            logger.warning("No valid captions, using first image")
            return images[0]
        
        if len(images_with_valid_caption) == 1:
            logger.info("Only one valid caption, auto-selected")
            return images_with_valid_caption[0]
        
        #  ì„ íƒì§€ ìƒì„±
        image_descriptions = []
        for choice_num, img in enumerate(images_with_valid_caption, 1):
            fig_num = img.get('figure_number', img['index'] + 1)
            caption = img.get('caption', '')
            
            desc = f"{choice_num}. (Figure {fig_num}): {caption} (Size: {img['size_kb']:.1f}KB)"
            image_descriptions.append(desc)
        
        #  ê°•í™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = f"""You are selecting the BEST figure for a research paper: "{paper_title}"

**TASK:** Choose the figure showing the paper's MAIN ARCHITECTURE or SYSTEM DESIGN.

**STRICT ELIMINATION RULES (Apply FIRST):**
âŒ REJECT if caption contains ANY of these keywords:
   - "Result", "Results", "Performance", "Accuracy", "Score"
   - "Comparison", "Compare", "Versus", "vs", "vs."
   - "Experiment", "Evaluation", "Benchmark", "Leaderboard"
   - "Ablation", "Analysis" (unless paired with "Architecture")
   - "Table", "Chart", "Graph" (unless about architecture)

**SELECTION PRIORITIES (After elimination):**
1. âœ… Keywords: "Architecture", "Framework", "System Design", "Workflow", "Pipeline", "Overview of method"
2. âœ… Descriptive captions explaining HOW the system works
3. âœ… Earlier figures (1-3) when tied

**IMPORTANT CLARIFICATIONS:**
- "Overall results" â†’ âŒ REJECT (has "results")
- "Overall architecture" â†’ âœ… GOOD (has "architecture")
- "Performance comparison" â†’ âŒ REJECT (has both!)
- "System overview" â†’ âœ… GOOD

**Figures:**
{chr(10).join(image_descriptions)}

**OUTPUT:** Respond with ONLY one number (1-{len(images_with_valid_caption)}). No explanation."""

        messages = [{"role": "user", "content": prompt}]
        
        #  max_tokens ì¦ê°€ (reasoning model ëŒ€ì‘)
        response = call_llm(messages, max_tokens=500)
        
        response_text = response.strip()
        logger.info(f"LLM response: '{response_text}'")
        
        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'\b(\d+)\b', response_text)
        
        if not numbers:
            logger.warning("No number in response, using first valid")
            return images_with_valid_caption[0]
        
        choice_num = int(numbers[0])
        choice_idx = choice_num - 1
        
        logger.info(f"LLM chose: choice={choice_num}, idx={choice_idx}")
        
        if 0 <= choice_idx < len(images_with_valid_caption):
            selected = images_with_valid_caption[choice_idx]
            
            logger.info("=" * 60)
            logger.info(f"âœ… SELECTED:")
            logger.info(f"   Index: {selected['index']}")
            logger.info(f"   Figure: {selected.get('figure_number', 'N/A')}")
            logger.info(f"   Caption: {selected.get('caption', '')[:80]}...")
            logger.info(f"   Size: {selected['size_kb']:.1f}KB")
            logger.info("=" * 60)
            
            return selected
        else:
            logger.warning(f"Choice {choice_num} out of range, using first")
            return images_with_valid_caption[0]
            
    except Exception as e:
        logger.error(f"Selection failed: {e}")
        logger.exception("Full traceback:")
        return images[0] if images else None


def select_representative_images(images: List[Dict], max_count: int = 1, paper_title: str = "") -> List[Dict]:
    """
    ë…¼ë¬¸ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ (Caption ìˆëŠ” ê²ƒë§Œ ê³ ë ¤)
    """
    if not images:
        return []
    
    if len(images) <= max_count:
        return images[:max_count]
    
    # LLMìœ¼ë¡œ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ (ë‚´ë¶€ì—ì„œ caption í•„í„°ë§)
    selected = select_representative_image_with_llm(images, paper_title)
    return [selected] if selected else []

# ===== LLM utils =====
def _estimate_tokens(s: str) -> int:
    """ê°„ë‹¨í•œ í† í° ì¶”ì • (1 token â‰ˆ 4 chars)"""
    return max(1, math.ceil(len(s) / 4))

def call_llm(messages: List[Dict], max_tokens: int = 2000) -> str:
    """LLM API í˜¸ì¶œ (reasoning model ì§€ì›)"""
    headers = {"Authorization": f"Bearer {LLM_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": LLM_MODEL, "messages": messages, "max_tokens": max_tokens, "temperature": 0.2}
    try:
        base_url = LLM_BASE_URL.rstrip('/') if LLM_BASE_URL else ""
        url = f"{base_url}/chat/completions"
        
        logger.info(f"Calling LLM: {url} (model: {LLM_MODEL}, max_tokens: {max_tokens})")
        
        r = requests.post(url, headers=headers, json=payload, timeout=120)
        r.raise_for_status()
        j = r.json()
        
        if "choices" not in j or not j["choices"]:
            raise ValueError(f"Invalid response: {list(j.keys())}")
        
        message = j["choices"][0].get("message", {})
        finish_reason = j["choices"][0].get("finish_reason")
        
        if finish_reason == "length":
            logger.warning(f"âš ï¸  Response truncated due to max_tokens limit!")
        
        content = (
            message.get("content") or 
            message.get("reasoning_content") or 
            message.get("text") or 
            ""
        )
        
        if not content.strip():
            logger.warning(f"Empty LLM response. finish_reason: {finish_reason}, message keys: {message.keys()}")
            
            if "reasoning_content" in message and not message.get("content"):
                logger.error(f"âŒ Only reasoning_content available, no actual answer!")
                logger.error(f"reasoning_content: {message['reasoning_content'][:200]}")
                logger.error(f"This usually means max_tokens is too low for reasoning models")
            
            return ""
        
        if "reasoning_content" in message and "content" not in message:
            logger.info("âš ï¸  Using reasoning_content (reasoning model, but content missing)")
        
        return content
        
    except Exception as e:
        logger.error(f"LLM call failed: {type(e).__name__}: {e}")
        raise
        
    except requests.exceptions.HTTPError as e:
        logger.error(f"LLM API HTTP Error: {e.response.status_code}")
        logger.error(f"Response: {e.response.text[:500]}")
        raise
    except ValueError as e:
        logger.error(f"LLM response format error: {e}")
        raise
    except KeyError as e:
        logger.error(f"LLM response missing key: {e}")
        raise
    except Exception as e:
        logger.error(f"LLM call failed: {type(e).__name__}: {e}")
        raise

def extract_all_headers(markdown_content: str) -> Dict[str, str]:
    """
    ì„¹ì…˜ ì´ë¦„ì— ê´€ê³„ì—†ì´ ì£¼ìš” í—¤ë”ë§Œ ì¶”ì¶œí•˜ì—¬ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬
    # (Level 1) ê³¼ ## (Level 2) í—¤ë”ë§Œ ì‚¬ìš© (### ì œì™¸)
    """
    logger.info("Extracting main headers (# and ##) from markdown...")
    sections = []
    current_section = {"header": "preamble", "level": 0, "content": []}
    
    for line in markdown_content.splitlines():
        # í—¤ë” ê°ì§€ (# ë˜ëŠ” ## ë§Œ, ### ì œì™¸!)
        header_match = re.match(r'^(#{1,2})\s+(.+)$', line.strip())
        
        if header_match:
            # ì´ì „ ì„¹ì…˜ ì €ì¥
            if current_section["content"]:
                sections.append(current_section)
            
            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            level = len(header_match.group(1))
            header = header_match.group(2).strip()
            current_section = {
                "header": header,
                "level": level,
                "content": []
            }
        else:
            current_section["content"].append(line)
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
    if current_section["content"]:
        sections.append(current_section)
    
    # Dictë¡œ ë³€í™˜ (í‚¤: section_N_header)
    result = {}
    for i, sec in enumerate(sections):
        # í—¤ë”ë¥¼ í‚¤ë¡œ ì‚¬ìš© (ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ë²ˆí˜¸ ì¶”ê°€)
        header_clean = re.sub(r'[^\w\s-]', '', sec['header'])[:30]
        key = f"section_{i:02d}_{header_clean}"
        content = "\n".join(sec["content"])
        
        if content.strip():  # ë¹ˆ ì„¹ì…˜ ì œì™¸
            result[key] = content
            logger.info(f"Section '{sec['header']}': {len(content)} chars, ~{_estimate_tokens(content)} toks")
    
    return result

def chunk_by_tokens(text: str, chunk_size: int = 4000, overlap: int = 400) -> List[str]:
    """
    í† í° ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ê· ë“± ë¶„í• 
    - ì„¹ì…˜ êµ¬ì¡°ê°€ ì—†ê±°ë‚˜ ë¶ˆëª…í™•í•  ë•Œ ì‚¬ìš©
    """
    logger.info(f"Chunking by tokens (chunk_size={chunk_size}, overlap={overlap})...")
    
    total_chars = len(text)
    if total_chars <= chunk_size * 4:  # í•œ ì²­í¬ì— ë“¤ì–´ê°
        return [text]
    
    # ëŒ€ëµì ì¸ ë¬¸ì ìˆ˜ ê³„ì‚° (1 token â‰ˆ 4 chars)
    char_per_chunk = chunk_size * 4
    overlap_chars = overlap * 4
    
    chunks = []
    start = 0
    
    while start < total_chars:
        end = start + char_per_chunk
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        if end < total_chars:
            # ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ìë¥´ê¸°
            for i in range(end, max(start + char_per_chunk // 2, end - 500), -1):
                if i < len(text) and text[i] in '.!?\n':
                    end = i + 1
                    break
        
        chunk = text[start:end]
        chunks.append(chunk)
        
        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (overlap ì ìš©)
        start = end - overlap_chars
        
        if start >= total_chars:
            break
    
    logger.info(f"Created {len(chunks)} chunks by tokens")
    return chunks

def smart_chunk_hybrid(markdown_content: str, min_headers: int = 10) -> Tuple[Dict[str, str], str]:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹: í—¤ë” ê°œìˆ˜ì— ë”°ë¼ ë°©ì‹ ì„ íƒ
    
    ê¸°ë³¸ê°’ì„ 8â†’10ìœ¼ë¡œ ìƒí–¥ ì¡°ì •í•˜ì—¬ ì²­í¬ ìˆ˜ ë”ìš± ê°ì†Œ
    # (Level 1) í—¤ë”ë§Œ ì¹´ìš´íŠ¸ (## ì œì™¸)
    
    Returns:
        (chunks_dict, method_used)
        method_used: "header_based" or "token_based"
    """
    # ë©”ì¸ í—¤ë”ë§Œ ì¹´ìš´íŠ¸ (# ë§Œ, ## ì œì™¸)
    main_headers = re.findall(r'^#\s+[^#].+$', markdown_content, re.MULTILINE)
    num_headers = len(main_headers)
    
    logger.info(f"Found {num_headers} main headers (# only) in markdown")
    
    if num_headers >= min_headers:
        # ì¶©ë¶„í•œ í—¤ë” â†’ í—¤ë” ê¸°ë°˜ ì„¹ì…˜ ë¶„ë¦¬ (í•˜ì§€ë§Œ ê±°ì˜ ì—†ì„ ê²ƒ)
        logger.info(f"Using HEADER-BASED chunking ({num_headers} headers >= {min_headers})")
        chunks = extract_all_headers(markdown_content)
        return chunks, "header_based"
    else:
        # í—¤ë” ë¶€ì¡± â†’ í† í° ê¸°ë°˜ ê· ë“± ë¶„í•  (ëŒ€ë¶€ë¶„ ì´ìª½)
        logger.info(f"Using TOKEN-BASED chunking ({num_headers} headers < {min_headers})")
        chunk_list = chunk_by_tokens(markdown_content, chunk_size=6000, overlap=600)
        chunks = {f"chunk_{i:02d}": chunk for i, chunk in enumerate(chunk_list)}
        return chunks, "token_based"

# ===== ê°œì„ ëœ ìš”ì•½ í•¨ìˆ˜ë“¤ =====

def smart_chunk_with_overlap(text: str, chunk_size: int = 4000, overlap: int = 400) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ overlapì„ ê°€ì§€ê³  ì²­í¬ë¡œ ë¶„í• 
    - chunk_size: ê° ì²­í¬ì˜ ëŒ€ëµì ì¸ í¬ê¸° (ë¬¸ì ë‹¨ìœ„)
    - overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ (ë¬¸ì ë‹¨ìœ„)
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ overlap ì ìš©
        if end < len(text):
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            chunk_end = end
            # ë§ˆì¹¨í‘œ, ì¤„ë°”ê¿ˆ ë“±ì„ ì°¾ì•„ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ ì°¾ê¸°
            for i in range(end, max(start + chunk_size // 2, end - 200), -1):
                if text[i] in '.!?\n':
                    chunk_end = i + 1
                    break
            chunks.append(text[start:chunk_end])
            start = chunk_end - overlap  # overlapë§Œí¼ ë’¤ë¡œ
        else:
            chunks.append(text[start:])
            break
    
    logger.info(f"Split into {len(chunks)} chunks with overlap={overlap}")
    return chunks

def summarize_chunk_with_overlap(
    chunk_key: str, 
    chunk_content: str, 
    paper_title: str,
    use_overlap: bool = True,
    prev_summary: str = ""
) -> str:
    """
    ì²­í¬ë¥¼ overlapì„ ê°€ì§€ê³  ìš”ì•½ (ì„¹ì…˜ ì´ë¦„ ë¬´ê´€)
    - chunk_key: "section_01_abstract" ë˜ëŠ” "chunk_00" í˜•ì‹
    - prev_summary: ì´ì „ ì²­í¬ì˜ ìš”ì•½ (contextë¡œ ì‚¬ìš©)
    """
    if not chunk_content.strip():
        return ""
    
    # ì²­í¬ íƒ€ì…ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ì¡°ì •
    if chunk_key.startswith("section_"):
        # ì„¹ì…˜ ê¸°ë°˜: í—¤ë” ì´ë¦„ ì¶”ì¶œ
        header_part = chunk_key.split("_", 2)[-1] if "_" in chunk_key else "content"
        prompt = f"ë‹¤ìŒ '{header_part}' ì„¹ì…˜ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”."
    else:
        # í† í° ê¸°ë°˜: ìˆœì„œë§Œ í‘œì‹œ
        chunk_num = chunk_key.split("_")[-1] if "_" in chunk_key else "0"
        prompt = f"ë…¼ë¬¸ì˜ ì¼ë¶€ë¶„ (Part {chunk_num})ì„ ìš”ì•½í•˜ì„¸ìš”."
    
    budget_tokens = min(LLM_MAX_TOKENS - 800, 6000)
    approx_tokens = _estimate_tokens(chunk_content)
    
    # ì²­í¬ê°€ ë„ˆë¬´ í¬ë©´ ë‹¤ì‹œ ë¶„í• 
    if approx_tokens <= budget_tokens:
        # í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
        context_prompt = ""
        if prev_summary and use_overlap:
            context_prompt = f"\nì´ì „ ë‚´ìš© ìš”ì•½: {prev_summary}\n"
        
        msgs = [
            {"role": "system", "content": "You are an expert AI paper analyst. Keep technical terms in English."},
            {"role": "user", "content": f"[{paper_title}] {prompt}{context_prompt}\n\në‚´ìš©:\n{chunk_content}"},
        ]
        return call_llm(msgs, max_tokens=min(3000, LLM_MAX_TOKENS - 500))
    
    # ë„ˆë¬´ í¬ë©´ sub-chunkë¡œ ë¶„í• 
    chunk_size_chars = budget_tokens * 4
    overlap_chars = 400 if use_overlap else 0
    
    sub_chunks = smart_chunk_with_overlap(chunk_content, chunk_size_chars, overlap_chars)
    
    summaries = []
    sub_prev_summary = prev_summary
    
    for i, sub_chunk in enumerate(sub_chunks):
        context_prompt = ""
        if sub_prev_summary and use_overlap:
            context_prompt = f"\nì´ì „ ë‚´ìš©: {sub_prev_summary}\n"
        
        msgs = [
            {"role": "system", "content": "You are an expert AI paper analyst. Keep technical terms in English."},
            {"role": "user", "content": f"[{paper_title}] {prompt} (sub-part {i+1}/{len(sub_chunks)}){context_prompt}\n\n{sub_chunk}"},
        ]
        summary = call_llm(msgs, max_tokens=min(3000, LLM_MAX_TOKENS - 500))
        summaries.append(summary)
        sub_prev_summary = summary
    
    # ì—¬ëŸ¬ sub-chunk ìš”ì•½ì„ ë³‘í•©
    if len(summaries) == 1:
        return summaries[0]
    
    merge_msgs = [
        {"role": "system", "content": "You are an expert AI paper analyst."},
        {"role": "user", "content": f"ë‹¤ìŒì€ [{paper_title}]ì˜ '{chunk_key}' ë¶€ë¶„ì„ ì—¬ëŸ¬ sub-partë¡œ ë‚˜ëˆ  ìš”ì•½í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì˜ ì¼ê´€ëœ ìš”ì•½ìœ¼ë¡œ ë³‘í•©í•˜ì„¸ìš”:\n\n" + "\n\n---\n\n".join(summaries)},
    ]
    return call_llm(merge_msgs, max_tokens=3000)

def create_hierarchical_summary_v2(chunk_summaries: Dict[str, str], paper_title: str) -> Dict[str, str]:
    """
    ê³„ì¸µì  ìš”ì•½ ìƒì„± v2 (ìœ„ì¹˜ ê¸°ë°˜)
    Level 1: ì²­í¬ ìš”ì•½ (ì´ë¯¸ ì™„ë£Œ)
    Level 2: ìœ„ì¹˜ë³„ ê·¸ë£¹ ìš”ì•½ (beginning, middle, end)
    Level 3: ìµœì¢… í†µí•© ìš”ì•½
    
    ì„¹ì…˜ ì´ë¦„ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë…¼ë¬¸ì˜ ìœ„ì¹˜(ì•/ì¤‘ê°„/ë’¤)ë¡œ ê·¸ë£¹í•‘
    """
    logger.info("Creating hierarchical summary (position-based)...")
    
    num_chunks = len(chunk_summaries)
    if num_chunks == 0:
        return {}
    
    # Level 2: ìœ„ì¹˜ ê¸°ë°˜ ê·¸ë£¹í•‘
    items = list(chunk_summaries.items())
    
    if num_chunks <= 2:
        # ì²­í¬ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        groups = {"full": items}
    elif num_chunks <= 5:
        # ì²­í¬ê°€ ì ìœ¼ë©´ 2ê°œ ê·¸ë£¹
        mid_point = num_chunks // 2
        groups = {
            "beginning": items[:mid_point],
            "end": items[mid_point:]
        }
    else:
        # ì²­í¬ê°€ ë§ìœ¼ë©´ 3ê°œ ê·¸ë£¹
        third = num_chunks // 3
        groups = {
            "beginning": items[:third],
            "middle": items[third:third*2],
            "end": items[third*2:]
        }
    
    # ê·¸ë£¹ë³„ í”„ë¡¬í”„íŠ¸
    group_prompts = {
        "full": "ë…¼ë¬¸ì˜ ì „ì²´ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
        "beginning": "ë…¼ë¬¸ì˜ ë„ì…ë¶€ (ë°°ê²½, ë¬¸ì œ ì •ì˜, ëª©í‘œ, ê´€ë ¨ ì—°êµ¬)ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
        "middle": "ë…¼ë¬¸ì˜ í•µì‹¬ ë¶€ë¶„ (ë°©ë²•ë¡ , ì‹¤í—˜ ì„¤ê³„, ê²°ê³¼, ì„±ëŠ¥)ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.",
        "end": "ë…¼ë¬¸ì˜ ê²°ë¡  ë¶€ë¶„ (ì¸ì‚¬ì´íŠ¸, í•œê³„, ê¸°ì—¬, í–¥í›„ ê³¼ì œ)ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”."
    }
    
    intermediate_summaries = {}
    for group_name, group_items in groups.items():
        if not group_items:
            continue
        
        # ê·¸ë£¹ ë‚´ ìš”ì•½ë“¤ì„ ê²°í•©
        group_texts = [f"### Part {i+1}\n{summary}" for i, (key, summary) in enumerate(group_items)]
        combined = "\n\n".join(group_texts)
        
        prompt = group_prompts.get(group_name, "ë‹¤ìŒ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.")
        
        msgs = [
            {"role": "system", "content": "You are an expert AI paper analyst."},
            {"role": "user", "content": f"[{paper_title}] {prompt}\n\n{combined}"},
        ]
        
        intermediate_summaries[group_name] = call_llm(msgs, max_tokens=3000)
        logger.info(f"Created intermediate summary for '{group_name}' ({len(group_items)} chunks)")
    
    return intermediate_summaries

def _json_extract(s: str) -> Optional[Dict]:
    """ë¬¸ìì—´ì—ì„œ JSON ì¶”ì¶œ"""
    m = re.search(r"\{[\s\S]*\}", s)
    if not m:
        return None
    try:
        return json.loads(m.group(0))
    except Exception:
        return None

def analyze_paper_with_llm_improved(
    paper_info: Dict, 
    markdown_content: str, 
    json_metadata: Dict,
    use_hierarchical: bool = True,
    use_overlap: bool = True,
    return_intermediate: bool = False
) -> Tuple[PaperAnalysis, Optional[Dict]]:
    """
    ê°œì„ ëœ ë…¼ë¬¸ ë¶„ì„ í•¨ìˆ˜ v2 (í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹)
    - í—¤ë” ë§ìœ¼ë©´ â†’ ì„¹ì…˜ ê¸°ë°˜
    - í—¤ë” ì ìœ¼ë©´ â†’ í† í° ê¸°ë°˜
    - use_hierarchical: ê³„ì¸µì  ìš”ì•½ ì‚¬ìš©
    - use_overlap: overlap chunking ì‚¬ìš©
    - return_intermediate: ì¤‘ê°„ ë‹¨ê³„ ê²°ê³¼ ë°˜í™˜
    """
    logger.info(f"Analyzing paper (hierarchical={use_hierarchical}, overlap={use_overlap}): {paper_info.get('title','Unknown')}")
    
    # Step 0: ì´ë¯¸ì§€ ì²˜ë¦¬ (LLM í† í° ì ˆì•½)
    clean_markdown, extracted_images = process_markdown_images(
        markdown_content, 
        remove_for_llm=True,
        keep_representative=1
    )
    
    if extracted_images:
        logger.info(f"Removed {len(extracted_images)} images for LLM processing")
    
    # Step 1: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (í•˜ì´ë¸Œë¦¬ë“œ) - clean_markdown ì‚¬ìš©
    chunks, chunking_method = smart_chunk_hybrid(clean_markdown, min_headers=10)
    logger.info(f"Chunking method: {chunking_method}, total chunks: {len(chunks)}")
    
    # Step 2: ê° ì²­í¬ ìš”ì•½
    chunk_summaries: Dict[str, str] = {}
    prev_summary = ""
    
    for chunk_key, content in chunks.items():
        if content.strip() and len(content.strip()) > 100:  # 100ì ì´ìƒë§Œ
            summary = summarize_chunk_with_overlap(
                chunk_key, 
                content, 
                paper_info.get("title", "Unknown"),
                use_overlap=use_overlap,
                prev_summary=prev_summary if use_overlap else ""
            )
            chunk_summaries[chunk_key] = summary
            prev_summary = summary
    
    logger.info(f"Created {len(chunk_summaries)} chunk summaries")
    
    # Step 3: ê³„ì¸µì  ìš”ì•½ (ì˜µì…˜)
    intermediate_summaries = {}
    if use_hierarchical and len(chunk_summaries) > 0:
        intermediate_summaries = create_hierarchical_summary_v2(
            chunk_summaries, 
            paper_info.get("title", "Unknown")
        )
        # ê³„ì¸µì  ìš”ì•½ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë¶„ì„
        combined = "\n\n".join([f"## {k.title()}\n{v}" for k, v in intermediate_summaries.items() if v.strip()])
    else:
        # ê¸°ì¡´ ë°©ì‹: ì²­í¬ ìš”ì•½ì„ ì§ì ‘ ê²°í•©
        combined = "\n\n".join([f"## {k}\n{v}" for k, v in chunk_summaries.items() if v.strip()])
    
    # Step 4: ìµœì¢… ì¢…í•© ë¶„ì„
    format_hint = {
        "title": paper_info.get("title", "Unknown"),
        "tldr": "",
        "key_contributions": [],
        "methodology": "",
        "results": "",
        "novelty": "",
        "limitations": [],
        "relevance_score": 7,
        "tags": [],
    }
    
    final_prompt = f"""ë…¼ë¬¸ "{paper_info.get('title','Unknown')}"ì˜ ì„¹ì…˜ë³„ ìš”ì•½ì´ ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤:

{combined}

ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ê²°ê³¼ë§Œ JSONìœ¼ë¡œ í•œê¸€ë¡œ ì¶œë ¥í•˜ì„¸ìš”(ì„¤ëª…ë¬¸ ê¸ˆì§€):
{json.dumps(format_hint, ensure_ascii=False, indent=2)}

ê·œì¹™:
- key_contributions: 3~6ê°œ bullet ìˆ˜ì¤€ì˜ ê°„ê²° ë¬¸ì¥
- relevance_score: **LLM ì—ì´ì „íŠ¸ ì—°êµ¬/ê°œë°œì— ëŒ€í•œ ê´€ë ¨ì„±** (1~10 ì •ìˆ˜)
  * 1-3: ê´€ë ¨ ì—†ìŒ (ì „í˜€ ë‹¤ë¥¸ ë¶„ì•¼ì˜ ì—°êµ¬)
  * 4-5: ê°„ì ‘ ê´€ë ¨ (ê¸°ì´ˆ ê¸°ìˆ ì´ë‚˜ ë°°ê²½ ì§€ì‹)
  * 6-7: ë³´í†µ ê´€ë ¨ (ì°¸ê³ í•  ë§Œí•œ ë°©ë²•ë¡ ì´ë‚˜ ì•„ì´ë””ì–´)
  * 8-9: ë†’ì€ ê´€ë ¨ì„± (ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ ê¸°ìˆ ì´ë‚˜ ë°©ë²•)
  * 10: í•„ìˆ˜ ì°¸ê³  (LLM ì—ì´ì „íŠ¸ì˜ í•µì‹¬ ê¸°ìˆ )

    íŠ¹íˆ ë‹¤ìŒ ì£¼ì œëŠ” ë†’ì€ ì ìˆ˜:
    - Agentic reasoning, tool use, planning
    - Reinforcement learning for LLM agents
    - Agent architectures, frameworks

- tags: 5~8ê°œ ì§§ì€ í‘œì œì–´ (ì˜ë¬¸)
- ì „ë¬¸ ìš©ì–´ëŠ” English ê·¸ëŒ€ë¡œ ìœ ì§€
"""
    
    msgs = [
        {"role": "system", "content": "You are an expert AI/ML researcher. Return ONLY valid JSON."},
        {"role": "user", "content": final_prompt},
    ]
    final_out = call_llm(msgs, max_tokens=min(3000, LLM_MAX_TOKENS))
    parsed = _json_extract(final_out) or {}
    
    # Abstract ì¶”ì¶œ (ì²« ë²ˆì§¸ ì²­í¬ì—ì„œ)
    abstract_text = ""
    for key, content in chunks.items():
        if len(content) < 2000:  # abstractëŠ” ë³´í†µ ì§§ìŒ
            abstract_text = content[:800]
            break
    
    analysis = PaperAnalysis(
        title=paper_info.get("title", "Unknown"),
        authors=paper_info.get("authors", []),
        abstract=abstract_text if abstract_text else "",
        summary=final_out if isinstance(final_out, str) else json.dumps(final_out, ensure_ascii=False),
        key_contributions=parsed.get("key_contributions", []),
        methodology=parsed.get("methodology", ""),
        results=parsed.get("results", ""),
        relevance_score=int(parsed.get("relevance_score", 7)),
        tags=parsed.get("tags", []),
        source_file=paper_info.get("s3_key", ""),
    )
    
    # ì¤‘ê°„ ë‹¨ê³„ ê²°ê³¼
    intermediate_data = None
    if return_intermediate:
        # ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ
        representative_imgs = select_representative_images(extracted_images, max_count=1)
        
        intermediate_data = {
            "chunking_method": chunking_method,
            "num_chunks": len(chunks),
            "chunks_detected": list(chunks.keys()),
            "chunk_summaries": chunk_summaries,
            "intermediate_summaries": intermediate_summaries if use_hierarchical else {},
            "images": {
                "total_count": len(extracted_images),
                "removed_size": sum(img['size'] for img in extracted_images),
                "representative": [
                    {
                        "index": img['index'],
                        "alt": img['alt'],
                        "type": img['type'],
                        "size": img['size'],
                        "markdown": img['full']
                    }
                    for img in representative_imgs
                ]
            }
        }
    
    return analysis, intermediate_data

# ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€
def analyze_paper_with_llm(paper_info: Dict, markdown_content: str, json_metadata: Dict) -> PaperAnalysis:
    """ê¸°ì¡´ í•¨ìˆ˜ (ê°œì„  ë²„ì „ í˜¸ì¶œ)"""
    analysis, _ = analyze_paper_with_llm_improved(
        paper_info, markdown_content, json_metadata,
        use_hierarchical=True, use_overlap=True, return_intermediate=False
    )
    return analysis

# ===== Confluence =====
def _conf_get_page_by_title(title: str) -> Optional[Dict]:
    url = f"{CONFLUENCE_URL}/rest/api/content"
    params = {"title": title, "spaceKey": CONFLUENCE_SPACE_KEY, "expand": "version"}
    r = requests.get(url, params=params, auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=30)
    r.raise_for_status()
    res = r.json().get("results", [])
    return res[0] if res else None

def _conf_escape(s: str) -> str:
    return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def upload_to_confluence(analyses: List[PaperAnalysis], page_title: str):
    logger.info(f"Uploading to Confluence: {page_title}")
    body = [f"<h1>AI Paper Newsletter - {datetime.now().strftime('%Y-%m-%d')}</h1>",
            "<p>ì´ë²ˆ ì£¼ì˜ ì£¼ëª©í•  ë§Œí•œ AI ë…¼ë¬¸ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.</p>",
            '<ac:structured-macro ac:name="info"><ac:rich-text-body>',
            f"<p>ì´ {len(analyses)}í¸ì˜ ë…¼ë¬¸ì´ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.</p>",
            "</ac:rich-text-body></ac:structured-macro><hr/>"]
    for i, a in enumerate(analyses, 1):
        body.append(f"<h2>{i}. {_conf_escape(a.title)}</h2>")
        if a.authors:
            body.append(f"<p><strong>Authors:</strong> {_conf_escape(', '.join(a.authors[:8]))}</p>")
        if a.tags:
            body.append(f"<p><strong>Tags:</strong> {_conf_escape(', '.join(a.tags))}</p>")
        if a.abstract:
            body.append("<h3>Abstract</h3><p>" + _conf_escape(a.abstract) + "</p>")
        body.append("<h3>Analysis</h3>")
        body.append(a.summary)
        body.append(f"<p><em>Source:</em> s3://{a.source_file}</p>")
        body.append("<hr/>")
    content_html = "\n".join(body)

    create_url = f"{CONFLUENCE_URL}/rest/api/content"
    headers = {"Content-Type": "application/json"}
    try:
        existing = _conf_get_page_by_title(page_title)
        if existing:
            page_id = existing["id"]
            version = existing.get("version", {}).get("number", 1) + 1
            payload = {
                "id": page_id, "type": "page", "title": page_title,
                "space": {"key": CONFLUENCE_SPACE_KEY},
                "body": {"storage": {"value": content_html, "representation": "storage"}},
                "version": {"number": version},
            }
            r = requests.put(f"{create_url}/{page_id}", json=payload, headers=headers,
                             auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=60)
            r.raise_for_status()
            result = r.json()
        else:
            payload = {
                "type": "page", "title": page_title, "space": {"key": CONFLUENCE_SPACE_KEY},
                "body": {"storage": {"value": content_html, "representation": "storage"}},
            }
            r = requests.post(create_url, json=payload, headers=headers,
                              auth=(CONFLUENCE_EMAIL, CONFLUENCE_API_TOKEN), timeout=60)
            r.raise_for_status()
            result = r.json()

        base = CONFLUENCE_URL.rstrip("/")
        webui = result.get("_links", {}).get("webui")
        tiny = result.get("_links", {}).get("tinyui")
        page_url = f"{base}{webui}" if webui else (f"{base}{tiny}" if tiny else f"{base}/pages/{result['id']}")
        logger.info(f"Confluence page: {page_url}")
        return {"success": True, "page_url": page_url, "page_id": result["id"]}
    except Exception as e:
        logger.exception("Confluence upload error")
        raise

# ===== Markdown builder =====
def derive_week_label(prefix: str) -> str:
    m = re.search(r"w(\d{1,2})", prefix or "", re.IGNORECASE)
    if m:
        return f"w{int(m.group(1))}"
    iso_year, iso_week, _ = datetime.utcnow().isocalendar()
    return f"w{iso_week}"

def build_markdown(
    analyses: List[PaperAnalysis], 
    papers_metadata: Optional[List[Dict]] = None,
    week_label: str = "", 
    prefix: str = ""
) -> Tuple[str, str]:
    """
    ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
    """
    if not week_label:
        week_label = derive_week_label(prefix)
    
    header = f"""# AI Paper Newsletter â€“ {week_label}
_Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_

Source prefix: `{prefix}`

---

"""
    
    # ì´ë¯¸ì§€ ë§¤í•‘ ìƒì„±
    image_map = {}
    if papers_metadata:
        for meta in papers_metadata:
            if meta.get('images_info') and meta.get('images_info', {}).get('representative_images'):
                s3_key = meta.get('s3_key', '')
                image_map[s3_key] = meta['images_info']
    
    parts = [header]
    
    for i, a in enumerate(analyses, 1):
        tags = f"**Tags:** {', '.join(a.tags)}" if a.tags else ""
        authors = f"**Authors:** {', '.join(a.authors[:8])}" if a.authors else ""
        
        abstract_block = ""
        if a.abstract and a.abstract.strip():
            abstract_block = f"\n**Abstract**\n\n> {a.abstract.strip()}\n\n"
        
        # Summary JSON íŒŒì‹± ë° ê°œì¡°ì‹ ë³€í™˜
        summary_formatted = format_summary_as_markdown(a.summary)
        
        sec = f"""## {i}. {a.title}

{authors}
{tags}

{summary_formatted}

{abstract_block}"""
        
        # ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
        if a.source_file in image_map:
            img_info = image_map[a.source_file]
            rep_imgs = img_info.get('representative_images', [])
            
            if rep_imgs:
                rep_img = rep_imgs[0]
                paper_name = Path(a.source_file).stem
                img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{rep_img['type']}"
                
                sec += f"""### ğŸ“Š ëŒ€í‘œ ì´ë¯¸ì§€

**ì „ì²´ ì´ë¯¸ì§€:** {img_info['total_images']}ê°œ  
**ëŒ€í‘œ ì´ë¯¸ì§€:** Figure {rep_img['index'] + 1} ({rep_img['size_kb']:.1f}KB)

![Figure {rep_img['index'] + 1}](images/{img_filename})

"""
        
        sec += f"""**Source:** `s3://{a.source_file}`

---

"""
        parts.append(sec)
    
    md_content = "".join(parts)
    md_filename = f"{week_label}.md"
    return md_filename, md_content


def format_summary_as_markdown(summary: str) -> str:
    """
    Summary JSONì„ ë³´ê¸° ì¢‹ì€ Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        summary: JSON í˜•íƒœì˜ summary ë¬¸ìì—´
    
    Returns:
        í¬ë§·íŒ…ëœ Markdown ë¬¸ìì—´
    """
    try:
        # JSON ì¶”ì¶œ ì‹œë„
        summary_clean = summary.strip().replace('~', 'â€“')
        
        # JSON íŒŒì‹±
        json_match = re.search(r'\{[\s\S]*\}', summary_clean)
        if not json_match:
            # JSONì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return f"**Summary**\n\n{summary_clean}\n"
        
        data = json.loads(json_match.group(0))
        
        # Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
        lines = ["**Summary**  \n\n"]
        
        # TL;DR
        if data.get('tldr'):
            lines.append(f"**ğŸ“Œ TL;DR**\n")
            lines.append(f"{data['tldr']}\n\n")
        
        # í•µì‹¬ ê¸°ì—¬
        if data.get('key_contributions'):
            lines.append(f"**ğŸ¯ í•µì‹¬ ê¸°ì—¬**\n")
            for contrib in data['key_contributions']:
                lines.append(f"- {contrib}\n")
            lines.append("\n")
        
        # ë°©ë²•ë¡ 
        if data.get('methodology'):
            lines.append(f"**ğŸ”¬ ë°©ë²•ë¡ **\n")
            lines.append(f"{data['methodology']}\n\n")
        
        # ê²°ê³¼
        if data.get('results'):
            lines.append(f"**ğŸ“Š ê²°ê³¼**\n")
            lines.append(f"{data['results']}\n\n")
        
        # ìƒˆë¡œìš´ ì 
        if data.get('novelty'):
            lines.append(f"**ğŸ’¡ ìƒˆë¡œìš´ ì **\n")
            lines.append(f"{data['novelty']}\n\n")
        
        # í•œê³„ì 
        if data.get('limitations'):
            lines.append(f"**âš ï¸ í•œê³„ì **\n")
            for limitation in data['limitations']:
                lines.append(f"- {limitation}\n")
            lines.append("\n")
        
        # Relevance Score
        if data.get('relevance_score'):
            score = data['relevance_score']
            stars = '' * score
            lines.append(f"**ê´€ë ¨ì„± ì ìˆ˜:** {stars} ({score}/10)\n\n")
        
        return "".join(lines)
        
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        logger.warning(f"Failed to parse summary JSON: {e}")
        return f"**Summary**\n\n{summary.strip().replace('~', 'â€“')}\n"

# ===== Main Routes =====
@app.get("/")
async def root():
    return {
        "service": "AI Paper Newsletter Processor",
        "version": "2.0.0",
        "status": "running",
        "improvements": [
            "smart_hybrid_chunking (header-based or token-based)",
            "overlap_chunking",
            "hierarchical_summarization (position-based)"
        ],
        "available_endpoints": ["/health", "/list-s3-papers", "/process-s3-papers", "/debug/*"],
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat(), "mode": "S3-only (v2: hybrid chunking)"}

@app.get("/list-s3-papers")
async def list_s3_papers(bucket: Optional[str] = None, prefix: Optional[str] = None):
    """
    S3ì—ì„œ ë…¼ë¬¸ ëª©ë¡ ì¡°íšŒ
    - paper_list.txtê°€ ìˆìœ¼ë©´ URL ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    - ì—†ìœ¼ë©´ PDF íŒŒì¼ ëª©ë¡ ë°˜í™˜
    """
    bucket = bucket or S3_BUCKET
    prefix = prefix or S3_PAPERS_PREFIX
    
    try:
        paper_urls = get_paper_list_from_s3(bucket, prefix)
        
        if paper_urls:
            logger.info(f"Found paper_list.txt with {len(paper_urls)} URLs")
            papers = [
                {
                    "title": extract_title_from_url(url),
                    "s3_key": f"{prefix}/{extract_title_from_url(url)}.pdf",  # ê°€ìƒ ê²½ë¡œ
                    "url": url,
                    "source": "url_list",
                    "last_modified": None,
                    "size_bytes": 0
                }
                for url in paper_urls
            ]
            
            return {
                "bucket": bucket,
                "prefix": prefix,
                "papers_found": len(papers),
                "source": "url_list",
                "paper_list_found": True,
                "papers": [
                    {
                        "title": p["title"],
                        "s3_key": p["s3_key"],
                        "url": p.get("url"),
                        "source": p.get("source"),
                        "last_modified": p.get("last_modified"),
                        "size_bytes": p.get("size_bytes", 0)
                    }
                    for p in papers
                ],
            }
        else:
            logger.info("paper_list.txt not found, listing PDF files")
            papers = get_s3_papers(bucket, prefix)
            
            return {
                "bucket": bucket,
                "prefix": prefix,
                "papers_found": len(papers),
                "source": "pdf_files",
                "paper_list_found": False,
                "papers": [
                    {
                        "title": p["title"],
                        "s3_key": p["s3_key"],
                        "source": p.get("source", "s3"),
                        "last_modified": p["last_modified"],
                        "size_bytes": p.get("size_bytes", 0)
                    }
                    for p in papers
                ],
            }
            
    except Exception as e:
        logger.error(f"list_s3_papers error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-s3-papers")
async def process_s3_papers(request: S3PapersRequest):
    logger.info(f"Processing S3 papers (hierarchical={request.use_hierarchical}, overlap={request.use_overlap})")
    bucket = request.bucket or S3_BUCKET
    prefix = request.prefix or S3_PAPERS_PREFIX
    
    try:
        # 1ë‹¨ê³„: paper_list.txt í™•ì¸
        paper_urls = get_paper_list_from_s3(bucket, prefix)
        
        if paper_urls:
            # URL ê¸°ë°˜ ì²˜ë¦¬
            logger.info(f"Processing {len(paper_urls)} papers from paper_list.txt")
            papers = [
                {
                    "title": extract_title_from_url(url),
                    "url": url,
                    "source": "url",
                    "s3_key": f"{prefix}/{extract_title_from_url(url)}.pdf"  # ê°€ìƒ ê²½ë¡œ
                }
                for url in paper_urls
            ]
        else:
            # ê¸°ì¡´ PDF íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬
            logger.info("paper_list.txt not found, processing PDF files")
            papers = get_s3_papers(
                bucket=bucket,
                prefix=prefix,
                file_pattern=request.file_pattern or "*.pdf",
                process_subdirectories=request.process_subdirectories,
            )
        
        if not papers:
            return {
                "message": "No papers found",
                "papers_found": 0,
                "papers_processed": 0,
                "bucket": bucket,
                "prefix": prefix,
                "md_filename": None,
                "md_content": "",
            }

        def _process_one(p: Dict) -> Tuple[Optional[PaperAnalysis], Optional[Dict], Optional[str]]:
            """ë…¼ë¬¸ 1ê°œ ì²˜ë¦¬"""
            tmp = None
            try:
                # URL ê¸°ë°˜ vs íŒŒì¼ ê¸°ë°˜ ì²˜ë¦¬
                if p.get("source") == "url":
                    # URLì—ì„œ ì§ì ‘ íŒŒì‹±
                    md, meta = parse_pdf_with_docpamin_url(p["url"], p["title"])
                    
                    actual_title = meta.get('extracted_title', p["title"])
                    info = {
                        "title": actual_title,
                        "authors": [], 
                        "abstract": "", 
                        "s3_key": p["s3_key"]
                    }
                else:
                    # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ íŒŒì‹±
                    tmp = download_pdf_from_s3(p["s3_key"], p["s3_bucket"])
                    md, meta = parse_pdf_with_docpamin(tmp)
                    
                    actual_title = extract_title_from_markdown(md) if md else p["title"]
                    meta['extracted_title'] = actual_title
                    
                    info = {
                        "title": actual_title,
                        "authors": [], 
                        "abstract": "", 
                        "s3_key": p["s3_key"]
                    }
                
                a, _ = analyze_paper_with_llm_improved(
                    info, md, meta,
                    use_hierarchical=request.use_hierarchical,
                    use_overlap=request.use_overlap,
                    return_intermediate=False
                )
                a.source_file = p["s3_key"]
                return a, meta, None
                
            except Exception as e:
                return None, None, f"{p.get('title', 'unknown')}: {e}"
            finally:
                if tmp and os.path.exists(tmp):
                    try: os.unlink(tmp)
                    except Exception: pass

        analyses: List[PaperAnalysis] = []
        papers_metadata: List[Dict] = []
        errors: List[str] = []
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futs = [ex.submit(_process_one, p) for p in papers]
            for fut in as_completed(futs):
                a, meta, err = fut.result()
                if a: 
                    analyses.append(a)
                    if meta and meta.get('images_info'):
                        papers_metadata.append({
                            's3_key': a.source_file,
                            'title': a.title,
                            'images_info': meta['images_info']
                        })
                if err: 
                    errors.append(err)

        week_label = request.week_label or derive_week_label(prefix)
        md_filename, md_content = build_markdown(analyses, papers_metadata, week_label, prefix)

        confluence_result = None
        if request.upload_confluence and analyses:
            page_title = f"AI Paper Newsletter - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            confluence_result = upload_to_confluence(analyses, page_title)

        return {
            "message": "Processed",
            "papers_found": len(papers),
            "papers_processed": len(analyses),
            "errors": errors,
            "bucket": bucket,
            "prefix": prefix,
            "week_label": week_label,
            "md_filename": md_filename,
            "md_content": md_content,
            "papers_metadata": papers_metadata,
            "source": "url_list" if paper_urls else "pdf_files",
            "confluence_url": (confluence_result or {}).get("page_url"),
            "improvements_used": {
                "hierarchical": request.use_hierarchical,
                "overlap": request.use_overlap
            }
        }
    except Exception as e:
        logger.exception("process_s3_papers error")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch-process")
async def batch_process_papers(request: BatchProcessRequest):
    logger.info("Batch processing papers")
    bucket = request.bucket or S3_BUCKET
    prefix = request.prefix or S3_PAPERS_PREFIX
    try:
        papers = get_s3_papers(bucket, prefix)
        if not papers:
            return {"message": "No papers found in S3", "papers_processed": 0}

        analyses: List[PaperAnalysis] = []
        for p in papers:
            tmp = None
            try:
                tmp = download_pdf_from_s3(p["s3_key"], p["s3_bucket"])
                md, meta = parse_pdf_with_docpamin(tmp)
                info = {"title": p["title"], "authors": [], "abstract": "", "s3_key": p["s3_key"]}
                a = analyze_paper_with_llm(info, md, meta)
                a.source_file = p["s3_key"]
                a.tags = request.tags
                analyses.append(a)
            except Exception as e:
                logger.error(f"Error processing: {e}")
            finally:
                if tmp and os.path.exists(tmp):
                    try: os.unlink(tmp)
                    except Exception: pass

        page_title = request.confluence_page_title or f"AI Paper Review - {datetime.now().strftime('%Y-%m-%d')}"
        confluence_result = upload_to_confluence(analyses, page_title)
        return {
            "message": "Successfully batch processed papers",
            "papers_found": len(papers),
            "papers_processed": len(analyses),
            "confluence_url": confluence_result.get("page_url"),
        }
    except Exception as e:
        logger.exception("batch_process error")
        raise HTTPException(status_code=500, detail=str(e))

# ===== Debug Endpoints =====

@app.post("/debug/parse-file")
async def debug_parse_file(
    file: UploadFile = File(...),
    include_markdown: bool = Form(False),
    markdown_max_chars: int = Form(5000),
):
    """ë¡œì»¬ PDF íŒŒì¼ ì—…ë¡œë“œ â†’ Docpamin íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    suffix = Path(file.filename).suffix or ".pdf"
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    try:
        content = await file.read()
        tmp.write(content)
        tmp.close()

        md, meta = parse_pdf_with_docpamin(tmp.name)
        resp = {
            "filename": file.filename,
            "md_len": len(md),
            "md_preview": _preview(md, markdown_max_chars),
            "metadata": meta,
            "images_info": meta.get("images_info"),
            # ğŸ”„  Backwards compatibility: older scripts expect `json_metadata`
            #     while newer ones read `metadata`.
            "json_metadata": meta,
        }
        if include_markdown:
            resp["markdown"] = md
        return resp
    finally:
        try:
            if os.path.exists(tmp.name):
                os.unlink(tmp.name)
        except Exception:
            pass

@app.post("/debug/parse-s3")
async def debug_parse_s3(req: DebugParseS3Request):
    """S3 íŠ¹ì • key â†’ Docpamin íŒŒì‹± í…ŒìŠ¤íŠ¸"""
    bucket = req.bucket or S3_BUCKET
    if not bucket:
        raise HTTPException(status_code=400, detail="Bucket is required")
    tmp = None
    try:
        tmp = download_pdf_from_s3(s3_key=req.key, s3_bucket=bucket)
        md, meta = parse_pdf_with_docpamin(tmp)
        resp = {
            "bucket": bucket,
            "key": req.key,
            "md_len": len(md),
            "md_preview": _preview(md, req.markdown_max_chars),
            "metadata": meta,
            "images_info": meta.get("images_info"),
            # ğŸ”„  Maintain both keys so workflows consuming either continue working.
            "json_metadata": meta,
        }
        if req.include_markdown:
            resp["markdown"] = md
        return resp
    finally:
        try:
            if tmp and os.path.exists(tmp):
                os.unlink(tmp)
        except Exception:
            pass

@app.post("/debug/summarize-markdown")
async def debug_summarize_markdown(req: DebugSummarizeMarkdownRequest):
    """
    Markdown í…ìŠ¤íŠ¸ â†’ ì²­í¬/ìµœì¢… ìš”ì•½ í…ŒìŠ¤íŠ¸ (ê°œì„  ë²„ì „ v2)
    - use_hierarchical: ê³„ì¸µì  ìš”ì•½ ì‚¬ìš©
    - use_overlap: overlap chunking ì‚¬ìš©
    - show_intermediate_steps: ì¤‘ê°„ ë‹¨ê³„ ì¶œë ¥
    """
    if not req.markdown or not req.markdown.strip():
        raise HTTPException(status_code=400, detail="markdown is empty")

    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (í•˜ì´ë¸Œë¦¬ë“œ)
    chunks, chunking_method = smart_chunk_hybrid(req.markdown, min_headers=5)
    
    chunk_summaries = {}
    if req.include_section_summaries:
        prev_summary = ""
        for chunk_key, chunk_text in chunks.items():
            if not chunk_text.strip() or len(chunk_text.strip()) < 100:
                continue
            chunk_summaries[chunk_key] = summarize_chunk_with_overlap(
                chunk_key, chunk_text, req.title, 
                use_overlap=req.use_overlap,
                prev_summary=prev_summary if req.use_overlap else ""
            )
            prev_summary = chunk_summaries[chunk_key]

    final_analysis = None
    intermediate_data = None
    if req.include_final_analysis:
        paper_info = {"title": req.title, "authors": [], "abstract": "", "s3_key": ""}
        final, intermediate = analyze_paper_with_llm_improved(
            paper_info, req.markdown, {},
            use_hierarchical=req.use_hierarchical,
            use_overlap=req.use_overlap,
            return_intermediate=req.show_intermediate_steps
        )
        final_analysis = {
            "title": final.title,
            "authors": final.authors,
            "abstract": final.abstract,
            "summary": final.summary,
            "key_contributions": final.key_contributions,
            "methodology": final.methodology,
            "results": final.results,
            "relevance_score": final.relevance_score,
            "tags": final.tags,
        }
        intermediate_data = intermediate

    return {
        "title": req.title,
        "chunking_method": chunking_method,
        "chunks_detected": list(chunks.keys()),
        "chunk_summaries": chunk_summaries if req.include_section_summaries else {},
        "final_analysis": final_analysis,
        "intermediate_steps": intermediate_data if req.show_intermediate_steps else None,
        "markdown_preview": _preview(req.markdown, req.return_markdown_preview_chars),
        "improvements_used": {
            "hierarchical": req.use_hierarchical,
            "overlap": req.use_overlap
        }
    }

@app.post("/debug/summarize-sections")
async def debug_summarize_sections(req: DebugSummarizeSectionsRequest):
    """ì„¹ì…˜/ì²­í¬ dict â†’ ìš”ì•½ í…ŒìŠ¤íŠ¸ (ê°œì„  ë²„ì „)"""
    if not req.sections:
        raise HTTPException(status_code=400, detail="sections is empty")

    targets = req.only_sections or list(req.sections.keys())
    out: Dict[str, str] = {}
    prev_summary = ""
    
    for sec in targets:
        text = req.sections.get(sec, "")
        if not text.strip():
            continue
        out[sec] = summarize_chunk_with_overlap(
            sec, text, req.title, 
            use_overlap=req.use_overlap,
            prev_summary=prev_summary if req.use_overlap else ""
        )
        prev_summary = out[sec]

    return {
        "title": req.title,
        "summarized_sections": list(out.keys()),
        "summaries": out,
        "improvements_used": {
            "overlap": req.use_overlap
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("paper_processor:app", host="0.0.0.0", port=7070, reload=False, workers=2)

    