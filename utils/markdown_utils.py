"""
Markdown formatting utilities for paper newsletters
"""
import logging
import re
import json
import base64
import os
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Tuple

from models import PaperAnalysis

logger = logging.getLogger(__name__)


def derive_week_label(prefix: str) -> str:
    """
    prefixì—ì„œ ì£¼ì°¨ ë ˆì´ë¸”ì„ ì¶”ì¶œí•˜ê±°ë‚˜ í˜„ì¬ ì£¼ì°¨ë¥¼ ë°˜í™˜

    Args:
        prefix: S3 prefix ë˜ëŠ” ì£¼ì°¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ìì—´

    Returns:
        ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
    """
    m = re.search(r"w(\d{1,2})", prefix or "", re.IGNORECASE)
    if m:
        return f"w{int(m.group(1))}"
    iso_year, iso_week, _ = datetime.utcnow().isocalendar()
    return f"w{iso_week}"


def save_images_to_files(
    papers_metadata: Optional[List[Dict]],
    week_label: str,
    output_dir: str = "images"
) -> Dict[str, str]:
    """
    ëŒ€í‘œ ì´ë¯¸ì§€ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥

    Args:
        papers_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
        week_label: ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
        output_dir: ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬

    Returns:
        Dict[s3_key, saved_filename]: ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼ëª… ë§¤í•‘
    """
    if not papers_metadata:
        return {}

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    saved_images = {}

    for meta in papers_metadata:
        images_info = meta.get('images_info', {})
        rep_imgs = images_info.get('representative_images', [])

        if not rep_imgs:
            continue

        rep_img = rep_imgs[0]
        s3_key = meta.get('s3_key', '')

        # base64 ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        base64_data = rep_img.get('base64_data')
        if not base64_data:
            logger.warning(f"No base64 data for image in {s3_key}")
            continue

        # íŒŒì¼ëª… ìƒì„±
        paper_name = Path(s3_key).stem
        img_type = rep_img.get('type', 'png')
        img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{img_type}"
        img_path = os.path.join(output_dir, img_filename)

        try:
            # base64 ë””ì½”ë”© ë° íŒŒì¼ ì €ì¥
            img_bytes = base64.b64decode(base64_data)
            with open(img_path, 'wb') as f:
                f.write(img_bytes)

            saved_images[s3_key] = img_filename
            logger.info(f"Saved image: {img_path} ({len(img_bytes)} bytes)")

        except Exception as e:
            logger.error(f"Failed to save image for {s3_key}: {e}")

    return saved_images


def format_summary_as_markdown(summary: str) -> str:
    """
    Summary JSONì„ ë³´ê¸° ì¢‹ì€ Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        summary: JSON í˜•íƒœì˜ summary ë¬¸ìì—´

    Returns:
        í¬ë§·íŒ…ëœ Markdown ë¬¸ìì—´
    """
    try:
        # JSON ì¶”ì¶œ ì‹œë„
        summary_clean = summary.strip().replace('~', 'â€“')

        # JSON íŒŒì‹±
        json_match = re.search(r'\{[\s\S]*\}', summary_clean)
        if not json_match:
            # JSONì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return f"**Summary**\n\n{summary_clean}\n"

        data = json.loads(json_match.group(0))

        # Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
        lines = ["**Summary**  \n\n"]

        # TL;DR
        if data.get('tldr'):
            lines.append(f"**ğŸ“Œ TL;DR**\n\n")
            lines.append(f"{data['tldr']}\n\n")

        # í•µì‹¬ ê¸°ì—¬
        if data.get('key_contributions'):
            lines.append(f"**ğŸ¯ í•µì‹¬ ê¸°ì—¬**\n\n")
            lines.append("".join([f"- {contrib}\n" for contrib in data['key_contributions']]))
            lines.append("\n")

        # ë°©ë²•ë¡ 
        if data.get('methodology'):
            lines.append(f"**ğŸ”¬ ë°©ë²•ë¡ **\n\n")
            lines.append(f"{data['methodology']}\n\n")

        # ê²°ê³¼
        if data.get('results'):
            lines.append(f"**ğŸ“Š ê²°ê³¼**\n\n")
            lines.append(f"{data['results']}\n\n")

        # ìƒˆë¡œìš´ ì 
        if data.get('novelty'):
            lines.append(f"**ğŸ’¡ ìƒˆë¡œìš´ ì **\n\n")
            lines.append(f"{data['novelty']}\n\n")

        # í•œê³„ì 
        if data.get('limitations'):
            lines.append(f"**âš ï¸ í•œê³„ì **\n\n")
            lines.append("".join([f"- {limitation}\n" for limitation in data['limitations']]))
            lines.append("\n")

        # Relevance Score
        if data.get('relevance_score'):
            score = data['relevance_score']
            stars = 'â­' * score
            lines.append(f"**ê´€ë ¨ì„± ì ìˆ˜:** {stars} ({score}/10)\n\n")

        return "".join(lines)

    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        logger.warning(f"Failed to parse summary JSON: {e}")
        return f"**Summary**\n\n{summary.strip().replace('~', 'â€“')}\n"


def build_markdown(
    analyses: List[PaperAnalysis],
    papers_metadata: Optional[List[Dict]] = None,
    week_label: str = "",
    prefix: str = "",
    save_images: bool = True
) -> Tuple[str, str]:
    """
    ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜

    Args:
        analyses: PaperAnalysis ê°ì²´ ë¦¬ìŠ¤íŠ¸
        papers_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
        week_label: ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
        prefix: S3 prefix
        save_images: ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€

    Returns:
        Tuple[str, str]: (íŒŒì¼ëª…, Markdown ì½˜í…ì¸ )
    """
    if not week_label:
        week_label = derive_week_label(prefix)

    # ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (GitHub í‘œì‹œìš©)
    if save_images and papers_metadata:
        saved_images = save_images_to_files(papers_metadata, week_label)
        logger.info(f"Saved {len(saved_images)} images to files")

    header = f"""# AI Paper Newsletter â€“ {week_label}
_Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_

Source prefix: `{prefix}`

---

"""

    # ì´ë¯¸ì§€ ë§¤í•‘ ìƒì„±
    image_map = {}
    if papers_metadata:
        for meta in papers_metadata:
            if meta.get('images_info') and meta.get('images_info', {}).get('representative_images'):
                s3_key = meta.get('s3_key', '')
                image_map[s3_key] = meta['images_info']

    parts = [header]

    for i, a in enumerate(analyses, 1):
        tags = f"**Tags:** {', '.join(a.tags)}" if a.tags else ""
        authors = f"**Authors:** {', '.join(a.authors[:8])}" if a.authors else ""

        abstract_block = ""
        if a.abstract and a.abstract.strip():
            abstract_block = f"\n**Abstract**\n\n> {a.abstract.strip()}\n\n"

        # Summary JSON íŒŒì‹± ë° ê°œì¡°ì‹ ë³€í™˜
        summary_formatted = format_summary_as_markdown(a.summary)

        sec = f"""## {i}. {a.title}

{authors}
{tags}

{summary_formatted}

{abstract_block}"""

        # ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€
        if a.source_file in image_map:
            img_info = image_map[a.source_file]
            rep_imgs = img_info.get('representative_images', [])

            if rep_imgs:
                rep_img = rep_imgs[0]
                paper_name = Path(a.source_file).stem
                img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{rep_img['type']}"

                sec += f"""### ğŸ“Š ëŒ€í‘œ ì´ë¯¸ì§€

**ì „ì²´ ì´ë¯¸ì§€:** {img_info['total_images']}ê°œ
**ëŒ€í‘œ ì´ë¯¸ì§€:** Figure {rep_img['index'] + 1} ({rep_img['size_kb']:.1f}KB)

![Figure {rep_img['index'] + 1}](images/{img_filename})

"""

        sec += f"""**Source:** `s3://{a.source_file}`

---

"""
        parts.append(sec)

    md_content = "".join(parts)
    md_filename = f"{week_label}.md"
    return md_filename, md_content
