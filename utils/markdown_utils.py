"""
Markdown formatting utilities for paper newsletters
"""
import logging
import re
import json
import base64
import os
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict, Tuple
from io import BytesIO

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    logger.warning("PIL not available - image resizing disabled")

from models import PaperAnalysis

logger = logging.getLogger(__name__)


def derive_week_label(prefix: str) -> str:
    """
    prefixì—ì„œ ì£¼ì°¨ ë ˆì´ë¸”ì„ ì¶”ì¶œí•˜ê±°ë‚˜ í˜„ì¬ ì£¼ì°¨ë¥¼ ë°˜í™˜

    Args:
        prefix: S3 prefix ë˜ëŠ” ì£¼ì°¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ìì—´

    Returns:
        ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
    """
    m = re.search(r"w(\d{1,2})", prefix or "", re.IGNORECASE)
    if m:
        return f"w{int(m.group(1))}"
    iso_year, iso_week, _ = datetime.utcnow().isocalendar()
    return f"w{iso_week}"


def resize_image_base64(base64_data: str, max_width: int = 600, quality: int = 85) -> Tuple[str, int]:
    """
    base64 ì´ë¯¸ì§€ë¥¼ ë¦¬ì‚¬ì´ì§•í•˜ê³  ì••ì¶•

    Args:
        base64_data: ì›ë³¸ base64 ì´ë¯¸ì§€ ë°ì´í„°
        max_width: ìµœëŒ€ ë„ˆë¹„ (í”½ì…€)
        quality: JPEG í’ˆì§ˆ (1-100)

    Returns:
        (resized_base64, size_bytes): ë¦¬ì‚¬ì´ì§•ëœ base64ì™€ ë°”ì´íŠ¸ í¬ê¸°
    """
    if not PIL_AVAILABLE:
        # PILì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        return base64_data, len(base64_data) * 3 // 4

    try:
        # base64 ë””ì½”ë”©
        img_bytes = base64.b64decode(base64_data)
        img = Image.open(BytesIO(img_bytes))

        # ì´ë¯¸ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if img.width <= max_width:
            return base64_data, len(img_bytes)

        # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë¦¬ì‚¬ì´ì§•
        ratio = max_width / img.width
        new_height = int(img.height * ratio)
        img_resized = img.resize((max_width, new_height), Image.Resampling.LANCZOS)

        # RGBë¡œ ë³€í™˜ (RGBAì¸ ê²½ìš°)
        if img_resized.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', img_resized.size, (255, 255, 255))
            if img_resized.mode == 'P':
                img_resized = img_resized.convert('RGBA')
            background.paste(img_resized, mask=img_resized.split()[-1] if img_resized.mode == 'RGBA' else None)
            img_resized = background

        # JPEGë¡œ ì••ì¶•
        output = BytesIO()
        img_resized.save(output, format='JPEG', quality=quality, optimize=True)
        resized_bytes = output.getvalue()

        # base64 ì¸ì½”ë”©
        resized_base64 = base64.b64encode(resized_bytes).decode('utf-8')

        logger.info(f"Image resized: {len(img_bytes)} â†’ {len(resized_bytes)} bytes "
                   f"({img.width}x{img.height} â†’ {max_width}x{new_height})")

        return resized_base64, len(resized_bytes)

    except Exception as e:
        logger.error(f"Failed to resize image: {e}")
        return base64_data, len(base64_data) * 3 // 4


def save_images_to_files(
    papers_metadata: Optional[List[Dict]],
    week_label: str,
    output_dir: str = "images",
    create_thumbnails: bool = True
) -> Dict[str, str]:
    """
    ëŒ€í‘œ ì´ë¯¸ì§€ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥

    Args:
        papers_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
        week_label: ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
        output_dir: ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
        create_thumbnails: ì¸ë„¤ì¼ ë²„ì „ë„ ìƒì„±í• ì§€ ì—¬ë¶€ (ë©”ì¼ìš©)

    Returns:
        Dict[s3_key, saved_filename]: ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼ëª… ë§¤í•‘
    """
    if not papers_metadata:
        return {}

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    saved_images = {}

    for meta in papers_metadata:
        images_info = meta.get('images_info', {})
        rep_imgs = images_info.get('representative_images', [])

        if not rep_imgs:
            continue

        rep_img = rep_imgs[0]
        s3_key = meta.get('s3_key', '')

        # base64 ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        base64_data = rep_img.get('base64_data')
        if not base64_data:
            logger.warning(f"No base64 data for image in {s3_key}")
            continue

        # íŒŒì¼ëª… ìƒì„±
        paper_name = Path(s3_key).stem
        img_type = rep_img.get('type', 'png')
        img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{img_type}"
        img_path = os.path.join(output_dir, img_filename)

        try:
            # ì›ë³¸ ì´ë¯¸ì§€ ì €ì¥
            img_bytes = base64.b64decode(base64_data)
            with open(img_path, 'wb') as f:
                f.write(img_bytes)

            saved_images[s3_key] = img_filename
            logger.info(f"Saved image: {img_path} ({len(img_bytes)} bytes)")

            # ì¸ë„¤ì¼ ë²„ì „ ìƒì„± (ë©”ì¼ìš©)
            if create_thumbnails and PIL_AVAILABLE:
                thumb_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}_thumb.jpg"
                thumb_path = os.path.join(output_dir, thumb_filename)

                resized_base64, resized_size = resize_image_base64(base64_data, max_width=600, quality=85)
                thumb_bytes = base64.b64decode(resized_base64)

                with open(thumb_path, 'wb') as f:
                    f.write(thumb_bytes)

                logger.info(f"Saved thumbnail: {thumb_path} ({resized_size} bytes)")

        except Exception as e:
            logger.error(f"Failed to save image for {s3_key}: {e}")

    return saved_images


def _optimize_images_for_email(
    papers_metadata: List[Dict],
    max_size_kb: int = 950
) -> List[Dict]:
    """
    ì´ë©”ì¼ í¬ê¸° ì œí•œì„ ìœ„í•´ ì´ë¯¸ì§€ë“¤ì„ ìµœì í™”

    Args:
        papers_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ì€ ë³´ì¡´)
        max_size_kb: ìµœëŒ€ í—ˆìš© í¬ê¸° (KB)

    Returns:
        ìµœì í™”ëœ papers_metadata ë³µì‚¬ë³¸
    """
    if not PIL_AVAILABLE:
        logger.warning("PIL not available - skipping image optimization")
        return papers_metadata

    # ì›ë³¸ ë³´ì¡´ì„ ìœ„í•´ deep copy
    import copy
    optimized_metadata = copy.deepcopy(papers_metadata)

    # ì—¬ëŸ¬ í’ˆì§ˆ ë ˆë²¨ë¡œ ì‹œë„
    quality_levels = [85, 75, 65, 55, 45]
    max_widths = [600, 500, 400, 350, 300]

    for quality_idx, (quality, max_width) in enumerate(zip(quality_levels, max_widths)):
        total_image_size = 0

        # ëª¨ë“  ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•
        for meta in optimized_metadata:
            images_info = meta.get('images_info', {})
            rep_imgs = images_info.get('representative_images', [])

            if not rep_imgs:
                continue

            for rep_img in rep_imgs:
                base64_data = rep_img.get('base64_data')
                if not base64_data:
                    continue

                # ë¦¬ì‚¬ì´ì§•
                resized_base64, resized_size = resize_image_base64(
                    base64_data,
                    max_width=max_width,
                    quality=quality
                )

                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                rep_img['base64_data'] = resized_base64
                rep_img['size_kb'] = resized_size / 1024
                total_image_size += resized_size

        # í…ìŠ¤íŠ¸ í¬ê¸° ì¶”ì • (ì´ë¯¸ì§€ ì œì™¸í•œ ë§ˆí¬ë‹¤ìš´)
        estimated_text_size = 50 * 1024  # ì•½ 50KBë¡œ ì¶”ì • (í—¤ë”, ìš”ì•½ ë“±)
        total_size_kb = (total_image_size + estimated_text_size) / 1024

        logger.info(f"Optimization attempt {quality_idx + 1}: "
                   f"quality={quality}, max_width={max_width}, "
                   f"total_size={total_size_kb:.1f}KB")

        if total_size_kb <= max_size_kb:
            logger.info(f"âœ… Image optimization successful: {total_size_kb:.1f}KB / {max_size_kb}KB")
            return optimized_metadata

        # ì•„ì§ í¬ë©´ ë‹¤ìŒ í’ˆì§ˆ ë ˆë²¨ ì‹œë„
        if quality_idx < len(quality_levels) - 1:
            logger.warning(f"âš ï¸  Still too large ({total_size_kb:.1f}KB), trying lower quality...")
            # Deep copy ë‹¤ì‹œ ìˆ˜í–‰í•˜ì—¬ ì›ë³¸ì—ì„œ ì¬ì‹œì‘
            optimized_metadata = copy.deepcopy(papers_metadata)

    # ìµœì € í’ˆì§ˆë¡œë„ ì•ˆë˜ë©´ ê²½ê³ í•˜ê³  ë°˜í™˜
    logger.warning(f"âš ï¸  Could not reduce size below {max_size_kb}KB even with lowest quality")
    return optimized_metadata


def format_summary_as_markdown(summary: str) -> str:
    """
    Summary JSONì„ ë³´ê¸° ì¢‹ì€ Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        summary: JSON í˜•íƒœì˜ summary ë¬¸ìì—´

    Returns:
        í¬ë§·íŒ…ëœ Markdown ë¬¸ìì—´
    """
    try:
        # JSON ì¶”ì¶œ ì‹œë„
        summary_clean = summary.strip().replace('~', 'â€“')

        # JSON íŒŒì‹±
        json_match = re.search(r'\{[\s\S]*\}', summary_clean)
        if not json_match:
            # JSONì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return f"**Summary**\n\n{summary_clean}\n"

        data = json.loads(json_match.group(0))

        # Markdown ê°œì¡°ì‹ìœ¼ë¡œ ë³€í™˜
        lines = ["**Summary**  \n\n"]

        # TL;DR
        if data.get('tldr'):
            lines.append(f"**ğŸ“Œ TL;DR**\n\n")
            lines.append(f"{data['tldr']}\n\n")

        # í•µì‹¬ ê¸°ì—¬
        if data.get('key_contributions'):
            lines.append(f"**ğŸ¯ í•µì‹¬ ê¸°ì—¬**\n\n")
            lines.append("".join([f"- {contrib}\n" for contrib in data['key_contributions']]))
            lines.append("\n")

        # ë°©ë²•ë¡ 
        if data.get('methodology'):
            lines.append(f"**ğŸ”¬ ë°©ë²•ë¡ **\n\n")
            lines.append(f"{data['methodology']}\n\n")

        # ê²°ê³¼
        if data.get('results'):
            lines.append(f"**ğŸ“Š ê²°ê³¼**\n\n")
            lines.append(f"{data['results']}\n\n")

        # ìƒˆë¡œìš´ ì 
        if data.get('novelty'):
            lines.append(f"**ğŸ’¡ ìƒˆë¡œìš´ ì **\n\n")
            lines.append(f"{data['novelty']}\n\n")

        # í•œê³„ì 
        if data.get('limitations'):
            lines.append(f"**âš ï¸ í•œê³„ì **\n\n")
            lines.append("".join([f"- {limitation}\n" for limitation in data['limitations']]))
            lines.append("\n")

        # Relevance Score
        if data.get('relevance_score'):
            score = data['relevance_score']
            stars = 'â­' * score
            lines.append(f"**ê´€ë ¨ì„± ì ìˆ˜:** {stars} ({score}/10)\n\n")

        return "".join(lines)

    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        logger.warning(f"Failed to parse summary JSON: {e}")
        return f"**Summary**\n\n{summary.strip().replace('~', 'â€“')}\n"


def build_markdown(
    analyses: List[PaperAnalysis],
    papers_metadata: Optional[List[Dict]] = None,
    week_label: str = "",
    prefix: str = "",
    save_images: bool = True,
    include_images: bool = True,
    optimize_for_email: bool = False,
    max_email_size_kb: int = 950
) -> Tuple[str, str]:
    """
    ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜

    Args:
        analyses: PaperAnalysis ê°ì²´ ë¦¬ìŠ¤íŠ¸
        papers_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
        week_label: ì£¼ì°¨ ë ˆì´ë¸” (ì˜ˆ: "w42")
        prefix: S3 prefix
        save_images: ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€
        include_images: ì´ë¯¸ì§€ ì„¹ì…˜ì„ í¬í•¨í• ì§€ ì—¬ë¶€ (ë©”ì¼ìš©: False, GitHubìš©: True)
        optimize_for_email: ì´ë©”ì¼ í¬ê¸° ì œí•œì„ ìœ„í•´ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• ì—¬ë¶€
        max_email_size_kb: ìµœëŒ€ ì´ë©”ì¼ í¬ê¸° (KB)

    Returns:
        Tuple[str, str]: (íŒŒì¼ëª…, Markdown ì½˜í…ì¸ )
    """
    if not week_label:
        week_label = derive_week_label(prefix)

    # ì´ë©”ì¼ ìµœì í™”: ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§•
    if optimize_for_email and papers_metadata and PIL_AVAILABLE:
        papers_metadata = _optimize_images_for_email(papers_metadata, max_email_size_kb)

    # ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (GitHub í‘œì‹œìš©)
    if save_images and papers_metadata:
        saved_images = save_images_to_files(papers_metadata, week_label)
        logger.info(f"Saved {len(saved_images)} images to files")

    header = f"""# AI Paper Newsletter â€“ {week_label}
_Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_

Source prefix: `{prefix}`

---

"""

    # ì´ë¯¸ì§€ ë§¤í•‘ ìƒì„±
    image_map = {}
    if papers_metadata:
        for meta in papers_metadata:
            if meta.get('images_info') and meta.get('images_info', {}).get('representative_images'):
                s3_key = meta.get('s3_key', '')
                image_map[s3_key] = meta['images_info']

    parts = [header]

    for i, a in enumerate(analyses, 1):
        tags = f"**Tags:** {', '.join(a.tags)}" if a.tags else ""
        authors = f"**Authors:** {', '.join(a.authors[:8])}" if a.authors else ""

        abstract_block = ""
        if a.abstract and a.abstract.strip():
            abstract_block = f"\n**Abstract**\n\n> {a.abstract.strip()}\n\n"

        # Summary JSON íŒŒì‹± ë° ê°œì¡°ì‹ ë³€í™˜
        summary_formatted = format_summary_as_markdown(a.summary)

        sec = f"""## {i}. {a.title}

{authors}
{tags}

{summary_formatted}

{abstract_block}"""

        # ì´ë¯¸ì§€ ì„¹ì…˜ ì¶”ê°€ (ì˜µì…˜)
        if include_images and a.source_file in image_map:
            img_info = image_map[a.source_file]
            rep_imgs = img_info.get('representative_images', [])

            if rep_imgs:
                rep_img = rep_imgs[0]
                paper_name = Path(a.source_file).stem
                img_filename = f"{week_label}_{paper_name}_fig{rep_img['index'] + 1}.{rep_img['type']}"

                sec += f"""### ğŸ“Š ëŒ€í‘œ ì´ë¯¸ì§€

**ì „ì²´ ì´ë¯¸ì§€:** {img_info['total_images']}ê°œ
**ëŒ€í‘œ ì´ë¯¸ì§€:** Figure {rep_img['index'] + 1} ({rep_img['size_kb']:.1f}KB)

![Figure {rep_img['index'] + 1}](images/{img_filename})

"""

        sec += f"""**Source:** `s3://{a.source_file}`

---

"""
        parts.append(sec)

    md_content = "".join(parts)
    md_filename = f"{week_label}.md"
    return md_filename, md_content
